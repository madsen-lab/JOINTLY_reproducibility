{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33093ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: R_HOME=/work/gpt/lib/R\n"
     ]
    }
   ],
   "source": [
    "%env R_HOME=/work/gpt/lib/R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd1f9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/work/gpt/lib/python3.8/site-packages/scanpy/_settings.py:447: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n"
     ]
    }
   ],
   "source": [
    "### Imports\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(4, 4))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ed02b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ucloud/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ffe2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define functions\n",
    "def _digitize(x: np.ndarray, bins: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Digitize the data into bins. This method spreads data uniformly when bins\n",
    "    have same values.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    x (:class:`np.ndarray`):\n",
    "        The data to digitize.\n",
    "    bins (:class:`np.ndarray`):\n",
    "        The bins to use for digitization, in increasing order.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    :class:`np.ndarray`:\n",
    "        The digitized data.\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1 and bins.ndim == 1\n",
    "\n",
    "    left_digits = np.digitize(x, bins)\n",
    "    right_difits = np.digitize(x, bins, right=True)\n",
    "\n",
    "    rands = np.random.rand(len(x))  # uniform random numbers\n",
    "\n",
    "    digits = rands * (right_difits - left_digits) + left_digits\n",
    "    digits = np.ceil(digits).astype(np.int64)\n",
    "    return digits\n",
    "    \n",
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    if sort_seq_batch:\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse, total_gepc = 0.0, 0.0, 0.0\n",
    "    total_error = 0.0\n",
    "    log_interval = config.log_interval\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if DSBN else None,\n",
    "                MVC=config.GEPC,\n",
    "                ECS=config.ecs_thres > 0,\n",
    "            )\n",
    "\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = loss_mse = criterion(\n",
    "                output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "            )\n",
    "            metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            if explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            if config.GEPC:\n",
    "                loss_gepc = criterion(\n",
    "                    output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_gepc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_gepc.item()})\n",
    "            if config.GEPC and explicit_zero_prob:\n",
    "                loss_gepc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_gepc_zero_log_prob\n",
    "                metrics_to_log.update(\n",
    "                    {\"train/mvc_nzlp\": loss_gepc_zero_log_prob.item()}\n",
    "                )\n",
    "            if config.ecs_thres > 0:\n",
    "                loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "                loss = loss + loss_ecs\n",
    "                metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "            loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "            loss = loss + config.dab_weight * loss_dab\n",
    "            metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        wandb.log(metrics_to_log)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mre = masked_relative_error(\n",
    "                output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "            )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item()\n",
    "        total_gepc += loss_gepc.item() if config.GEPC else 0.0\n",
    "        total_error += mre.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_gepc = total_gepc / log_interval if config.GEPC else 0.0\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\"\n",
    "                + (f\"gepc {cur_gepc:5.2f} |\" if config.GEPC else \"\")\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_gepc = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def define_wandb_metrcis():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"test/avg_bio\", summary=\"max\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if DSBN else None,\n",
    "                )\n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = input_values.eq(mask_value)\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_error += masked_relative_error(\n",
    "                output_values, target_values, masked_positions\n",
    "            ).item() * len(input_gene_ids)\n",
    "            total_dab += loss_dab.item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mre\": total_error / total_num,\n",
    "            \"valid/dab\": total_dab / total_num,\n",
    "            \"valid/sum_mse_dab\": (total_loss + config.dab_weight * total_dab)\n",
    "            / total_num,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num\n",
    "\n",
    "\n",
    "def eval_testdata(\n",
    "    model: nn.Module,\n",
    "    adata_t: AnnData,\n",
    "    include_types: List[str] = [\"cls\"],\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"evaluate the model on test dataset of adata_t\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # copy adata_t to avoid reuse previously computed results stored in adata_t\n",
    "    adata_t = adata_t.copy()\n",
    "\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata_t.obs[\"celltype\"].tolist()\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Evaluate cls cell embeddings\n",
    "    if \"cls\" in include_types:\n",
    "        logger.info(\"Evaluating cls cell embeddings\")\n",
    "        tokenized_all = tokenize_and_pad_batch(\n",
    "            all_counts,\n",
    "            gene_ids,\n",
    "            max_len=max_seq_len,\n",
    "            vocab=vocab,\n",
    "            pad_token=pad_token,\n",
    "            pad_value=pad_value,\n",
    "            append_cls=True,  # append <cls> token at the beginning\n",
    "            include_zero_gene=True,\n",
    "        )\n",
    "        all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "        src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            cell_embeddings = model.encode_batch(\n",
    "                all_gene_ids,\n",
    "                all_values.float(),\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_size=config.batch_size,\n",
    "                batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "                time_step=0,\n",
    "                return_np=True,\n",
    "            )\n",
    "        cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "            cell_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "\n",
    "        adata_t.obsm[\"X_scGPT\"] = cell_embeddings\n",
    "\n",
    "        results = {}\n",
    "        try:\n",
    "            results = eval_scib_metrics(adata_t)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            logger.error(e)\n",
    "\n",
    "        sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "        sc.tl.umap(adata_t, min_dist=0.3)\n",
    "        fig = sc.pl.umap(\n",
    "            adata_t,\n",
    "            color=[\"str_batch\"],\n",
    "            title=[f\"batch, avg_bio = {results.get('avg_bio', 0.0):.4f}\"],\n",
    "            frameon=False,\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "        results[\"batch_umap\"] = fig\n",
    "\n",
    "        sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "        sc.tl.umap(adata_t, min_dist=0.3)\n",
    "        fig = sc.pl.umap(\n",
    "            adata_t,\n",
    "            color=[\"celltype\"],\n",
    "            title=[\n",
    "                f\"celltype, avg_bio = {results.get('avg_bio', 0.0):.4f}\",\n",
    "            ],\n",
    "            frameon=False,\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "        results[\"celltype_umap\"] = fig\n",
    "\n",
    "    if len(include_types) == 1:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b3763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pll9hzdz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/dab</td><td>▄▄▄▃▂▂▁▆▆▆▄▃▃▂▁▁▁█▇▆▆▅▄▂▂▁▆▆▆▅▄▂▂▁▁▁▁██▇</td></tr><tr><td>train/ecs</td><td>▂▂▂▁▁▅▅▄▅▄▄▅▄▃▃▃▄▄▆▇▇▅▄▅▅▆▆█▆█▆▆▆▆▆▆▆▆▇▆</td></tr><tr><td>train/mse</td><td>█▇█▇▆▆▆▇▇▆▆▇▆▆▆▆▆▅▄▄▃▃▂▂▂▂▃▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>train/mvc</td><td>█▇█▄▃▂▂▃▃▂▂▂▂▂▂▂▁▂▁▁▂▂▁▁▁▁▂▂▂▂▂▁▁▁▁▁▁▃▂▁</td></tr><tr><td>train/mvc_nzlp</td><td>██▇▃▄▃▂▆▅▃▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▅▅▄▃▂▁▁▁▁▁▁▂▂▁</td></tr><tr><td>train/nzlp</td><td>▃▃▂▂▃▂▃▃▂▂▃▃▂▃▂▃▂██▇█▇▆▅▅▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/dab</td><td>5.88318</td></tr><tr><td>train/ecs</td><td>9.68107</td></tr><tr><td>train/mse</td><td>118.59852</td></tr><tr><td>train/mvc</td><td>137.23468</td></tr><tr><td>train/mvc_nzlp</td><td>1.01268</td></tr><tr><td>train/nzlp</td><td>0.68715</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">easy-microwave-21</strong>: <a href=\"https://wandb.ai/madlab_sdu/scGPT/runs/pll9hzdz\" target=\"_blank\">https://wandb.ai/madlab_sdu/scGPT/runs/pll9hzdz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230828_223007-pll9hzdz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pll9hzdz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/NMF_project/reproducibility/wandb/run-20230828_223124-326p3hm7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/madlab_sdu/scGPT/runs/326p3hm7\" target=\"_blank\">sunny-cosmos-22</a></strong> to <a href=\"https://wandb.ai/madlab_sdu/scGPT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'Human_Liver', 'do_train': True, 'load_model': '/work/NMF_project/reproducibility/data/scGPT/Model/', 'GEPC': True, 'ecs_thres': 0.8, 'dab_weight': 1.0, 'mask_ratio': 0.4, 'epochs': 15, 'n_bins': 51, 'lr': 0.0001, 'batch_size': 64, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True}\n",
      "save to /work/NMF_project/reproducibility/data/scGPT/FT/dev_Human_Liver_0\n",
      "scGPT - INFO - match 24017/24334 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /work/NMF_project/reproducibility/data/scGPT/Model/best_model.pt, the model args will be overriden by the config /work/NMF_project/reproducibility/data/scGPT/Model/args.json.\n",
      "scGPT - INFO - train set number of samples: 7599, \n",
      "\t feature length: 1201\n",
      "scGPT - INFO - valid set number of samples: 845, \n",
      "\t feature length: 1201\n",
      "Use domain specific batchnorm with affine=False\n",
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.bias with shape torch.Size([512])\n",
      "random masking at epoch   1, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   1 | 100/121 batches | lr 0.0001 | ms/batch 303.10 | loss 371.58 | mse 183.36 | mre 4741911.17 |gepc 174.48 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 38.16s | valid loss/mse 162.9308 | mre 4688310.4901\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 162.9308\n",
      "random masking at epoch   2, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   2 | 100/121 batches | lr 0.0001 | ms/batch 308.02 | loss 228.48 | mse 96.45 | mre 2837797.83 |gepc 118.38 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 39.20s | valid loss/mse 87.9789 | mre 2916971.7885\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 87.9789\n",
      "random masking at epoch   3, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   3 | 100/121 batches | lr 0.0001 | ms/batch 307.90 | loss 198.41 | mse 84.63 | mre 2552363.30 |gepc 100.12 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 38.76s | valid loss/mse 79.0114 | mre 2130624.6780\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 79.0114\n",
      "random masking at epoch   4, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   4 | 100/121 batches | lr 0.0001 | ms/batch 305.80 | loss 202.59 | mse 78.77 | mre 2293512.63 |gepc 110.52 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 38.81s | valid loss/mse 80.6255 | mre 2446512.4351\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   5, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   5 | 100/121 batches | lr 0.0001 | ms/batch 310.67 | loss 206.26 | mse 77.51 | mre 2298602.75 |gepc 115.80 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 39.03s | valid loss/mse 77.7005 | mre 2709955.7231\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 77.7005\n",
      "random masking at epoch   6, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   6 | 100/121 batches | lr 0.0001 | ms/batch 308.52 | loss 200.67 | mse 77.09 | mre 2323690.15 |gepc 110.42 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 38.79s | valid loss/mse 79.2961 | mre 2821580.0740\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   7, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   7 | 100/121 batches | lr 0.0001 | ms/batch 305.01 | loss 174.34 | mse 76.32 | mre 2325763.65 |gepc 85.46 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 38.35s | valid loss/mse 74.9147 | mre 2235983.7923\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 74.9147\n",
      "random masking at epoch   8, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   8 | 100/121 batches | lr 0.0000 | ms/batch 304.94 | loss 177.02 | mse 75.05 | mre 2237556.37 |gepc 89.47 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 38.21s | valid loss/mse 74.9685 | mre 2485764.9234\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   9, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   9 | 100/121 batches | lr 0.0000 | ms/batch 315.01 | loss 177.70 | mse 74.74 | mre 2310678.45 |gepc 90.46 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 39.40s | valid loss/mse 74.0301 | mre 2136306.8703\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 74.0301\n",
      "random masking at epoch  10, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  10 | 100/121 batches | lr 0.0000 | ms/batch 314.05 | loss 173.04 | mse 74.03 | mre 2254400.92 |gepc 86.75 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 39.44s | valid loss/mse 72.4728 | mre 1796392.0680\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 72.4728\n",
      "random masking at epoch  11, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  11 | 100/121 batches | lr 0.0000 | ms/batch 323.74 | loss 178.02 | mse 73.87 | mre 2155198.81 |gepc 92.03 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 40.52s | valid loss/mse 73.6142 | mre 2084941.0960\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  12, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  12 | 100/121 batches | lr 0.0000 | ms/batch 321.21 | loss 166.11 | mse 73.70 | mre 2222849.06 |gepc 80.53 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 39.98s | valid loss/mse 74.2596 | mre 1869408.9340\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  13, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  13 | 100/121 batches | lr 0.0000 | ms/batch 308.34 | loss 162.66 | mse 73.06 | mre 2207819.54 |gepc 78.22 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 39.13s | valid loss/mse 71.8345 | mre 2093900.7451\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 71.8345\n",
      "random masking at epoch  14, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  14 | 100/121 batches | lr 0.0000 | ms/batch 318.49 | loss 162.63 | mse 72.49 | mre 2202179.69 |gepc 79.43 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 39.69s | valid loss/mse 71.3914 | mre 1973063.1963\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 71.3914\n",
      "random masking at epoch  15, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  15 | 100/121 batches | lr 0.0000 | ms/batch 313.83 | loss 160.16 | mse 72.27 | mre 2198533.28 |gepc 77.90 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 39.22s | valid loss/mse 71.9043 | mre 1898692.3146\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 71/132 [00:05<00:05, 12.06it/s]"
     ]
    }
   ],
   "source": [
    "## Define variables\n",
    "dataset_id = \"Human_Liver\"\n",
    "adata_path = \"/work/NMF_project/reproducibility/data/Liver/Human_Liver.h5ad\"\n",
    "hvg_path = \"/work/NMF_project/reproducibility/data/Liver/Human_Liver.features\"\n",
    "latent_path = \"/work/NMF_project/reproducibility/data/Liver/Human_Liver\"\n",
    "\n",
    "for rep in range(5):\n",
    "    ## Set hyperparameters\n",
    "    hyperparameter_defaults = dict(\n",
    "        seed=42,\n",
    "        dataset_name=dataset_id, # Dataset name\n",
    "        do_train=True, # Flag to indicate whether to do update model parameters during training\n",
    "        load_model=\"/work/NMF_project/reproducibility/data/scGPT/Model/\", # Path to pre-trained model\n",
    "        GEPC=True,  # Gene expression modelling for cell objective\n",
    "        ecs_thres=0.8,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "        dab_weight=1.0, # DAR objective weight for batch correction\n",
    "        mask_ratio=0.4, # Default mask ratio\n",
    "        epochs=15, # Default number of epochs for fine-tuning\n",
    "        n_bins=51, # Default number of bins for value binning in data pre-processing\n",
    "        lr=1e-4, # Default learning rate for fine-tuning\n",
    "        batch_size=64, # Default batch size for fine-tuning\n",
    "        layer_size=128,\n",
    "        nlayers=4,\n",
    "        nhead=4, # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "        dropout=0.2, # Default dropout rate during model fine-tuning\n",
    "        schedule_ratio=0.9,  # Default rate for learning rate decay\n",
    "        save_eval_interval=5, # Default model evaluation interval\n",
    "        log_interval=100, # Default log interval\n",
    "        fast_transformer=True, # Default setting\n",
    "        pre_norm=False, # Default setting\n",
    "        amp=True,  # # Default setting: Automatic Mixed Precision\n",
    "    )\n",
    "\n",
    "    ## Initialize the run on wandb\n",
    "    run = wandb.init(\n",
    "        config=hyperparameter_defaults,\n",
    "        project=\"scGPT\",\n",
    "        reinit=True,\n",
    "        settings=wandb.Settings(start_method=\"fork\"),\n",
    "    )\n",
    "    config = wandb.config\n",
    "    print(config)\n",
    "    set_seed(config.seed)\n",
    "\n",
    "    # Settings for input and preprocessing\n",
    "    pad_token = \"<pad>\"\n",
    "    special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "    mask_ratio = config.mask_ratio\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = config.n_bins\n",
    "    n_hvg = 1200  # number of highly variable genes\n",
    "    max_seq_len = n_hvg + 1\n",
    "    per_seq_batch_sample = True\n",
    "    DSBN = True  # Domain-spec batchnorm\n",
    "    explicit_zero_prob = True  # whether explicit bernoulli for zeros\n",
    "\n",
    "    # Settings for saving the model\n",
    "    dataset_name = config.dataset_name\n",
    "    save_dir = Path(f\"/work/NMF_project/reproducibility/data/scGPT/FT/dev_{dataset_name}_{rep}/\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"save to {save_dir}\")\n",
    "    logger = scg.logger\n",
    "    scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "\n",
    "    # Load data\n",
    "    adata = sc.read(adata_path)\n",
    "    ori_batch_col = \"batch_label\"\n",
    "    adata.var = adata.var.set_index(\"features\")\n",
    "    data_is_raw = True\n",
    "\n",
    "    # make the batch category column\n",
    "    adata.obs[\"str_batch\"] = adata.obs[ori_batch_col].astype(str)\n",
    "    batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "    adata.obs[\"batch_id\"] = batch_id_labels\n",
    "    adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "    # Define HVGs\n",
    "    f = open(hvg_path, \"r\")\n",
    "    hvg = f.read().splitlines()\n",
    "    f.close()\n",
    "    adata.var['highly_variable'] = [True if g in hvg else False for g in adata.var_names]\n",
    "\n",
    "    # Load the pretrained model\n",
    "    if config.load_model is not None:\n",
    "        model_dir = Path(config.load_model)\n",
    "        model_config_file = model_dir / \"args.json\"\n",
    "        model_file = model_dir / \"best_model.pt\"\n",
    "        vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "        vocab = GeneVocab.from_file(vocab_file)\n",
    "        for s in special_tokens:\n",
    "            if s not in vocab:\n",
    "                vocab.append_token(s)\n",
    "\n",
    "        adata.var[\"id_in_vocab\"] = [\n",
    "            1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "        ]\n",
    "        gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "        logger.info(\n",
    "            f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "            f\"in vocabulary of size {len(vocab)}.\"\n",
    "        )\n",
    "        adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "        # model\n",
    "        with open(model_config_file, \"r\") as f:\n",
    "            model_configs = json.load(f)\n",
    "        logger.info(\n",
    "            f\"Resume model from {model_file}, the model args will be overriden by the \"\n",
    "            f\"config {model_config_file}.\"\n",
    "        )\n",
    "        embsize = model_configs[\"embsize\"]\n",
    "        nhead = model_configs[\"nheads\"]\n",
    "        d_hid = model_configs[\"d_hid\"]\n",
    "        nlayers = model_configs[\"nlayers\"]\n",
    "        n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "    else:\n",
    "        embsize = config.layer_size\n",
    "        nhead = config.nhead\n",
    "        nlayers = config.nlayers\n",
    "        d_hid = config.layer_size\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    sc.pp.filter_genes(adata, min_counts=3)\n",
    "    normed = sc.pp.normalize_total(adata, target_sum=1e4, layer=None, inplace=False)[\"X\"]\n",
    "    sc.get._set_obs_rep(adata, normed, layer=\"X_normed\")\n",
    "    sc.get._set_obs_rep(adata,sc.get._get_obs_rep(adata, layer=\"X_normed\"), layer=\"X_log1p\")\n",
    "    sc.pp.log1p(adata, layer=\"X_log1p\")\n",
    "    adata = adata[:, adata.var.highly_variable]\n",
    "    n_bins = config.n_bins  # NOTE: the first bin is always a spectial for zero\n",
    "    binned_rows = []\n",
    "    bin_edges = []\n",
    "    layer_data = sc.get._get_obs_rep(adata, layer=\"X_log1p\")\n",
    "    layer_data = layer_data.A if issparse(layer_data) else layer_data\n",
    "    for row in layer_data:\n",
    "        non_zero_ids = row.nonzero()\n",
    "        non_zero_row = row[non_zero_ids]\n",
    "        bins = np.quantile(non_zero_row, np.linspace(0, 1, n_bins - 1))\n",
    "        non_zero_digits = _digitize(x = non_zero_row, bins = bins)\n",
    "        assert non_zero_digits.min() >= 1\n",
    "        assert non_zero_digits.max() <= n_bins - 1\n",
    "        binned_row = np.zeros_like(row, dtype=np.int64)\n",
    "        binned_row[non_zero_ids] = non_zero_digits\n",
    "        binned_rows.append(binned_row)\n",
    "        bin_edges.append(np.concatenate([[0], bins]))\n",
    "    adata.layers[\"X_binned\"] = np.stack(binned_rows)\n",
    "    adata.obsm[\"bin_edges\"] = np.stack(bin_edges)\n",
    "\n",
    "    # Sort the adata by batch_id in advance\n",
    "    if per_seq_batch_sample:\n",
    "        adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()\n",
    "\n",
    "    # Define input layers and get counts\n",
    "    input_layer_key = \"X_binned\"\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "    genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "    # Get batch ids\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    num_batch_types = len(set(batch_ids))\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Create splits\n",
    "    (\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        train_batch_labels,\n",
    "        valid_batch_labels,\n",
    "    ) = train_test_split(\n",
    "        all_counts, batch_ids, test_size=0.1, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Define vocabulary\n",
    "    if config.load_model is None:\n",
    "        vocab = Vocab(\n",
    "            VocabPybind(genes + special_tokens, None)\n",
    "        )  # bidirectional lookup [gene <-> int]\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)\n",
    "\n",
    "    # Tokenize training and validation data\n",
    "    tokenized_train = tokenize_and_pad_batch(\n",
    "        train_data,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    tokenized_valid = tokenize_and_pad_batch(\n",
    "        valid_data,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "        f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "        f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    "    )\n",
    "\n",
    "    # Load the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ntokens = len(vocab)  # size of vocabulary\n",
    "    model = TransformerModel(\n",
    "        ntokens,\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid,\n",
    "        nlayers,\n",
    "        vocab=vocab,\n",
    "        dropout=config.dropout,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        do_mvc=config.GEPC,\n",
    "        do_dab=True,\n",
    "        use_batch_labels=True,\n",
    "        num_batch_labels=num_batch_types,\n",
    "        domain_spec_batchnorm=DSBN,\n",
    "        n_input_bins=n_input_bins,\n",
    "        ecs_threshold=config.ecs_thres,\n",
    "        explicit_zero_prob=explicit_zero_prob,\n",
    "        use_fast_transformer=config.fast_transformer,\n",
    "        pre_norm=config.pre_norm,\n",
    "    )\n",
    "    if config.load_model is not None:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_file))\n",
    "            logger.info(f\"Loading all model params from {model_file}\")\n",
    "        except:\n",
    "            # only load params that are in the model and match the size\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = torch.load(model_file)\n",
    "            pretrained_dict = {\n",
    "                k: v\n",
    "                for k, v in pretrained_dict.items()\n",
    "                if k in model_dict and v.shape == model_dict[k].shape\n",
    "            }\n",
    "            for k, v in pretrained_dict.items():\n",
    "                logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    wandb.watch(model)\n",
    "\n",
    "    # Set model criteria\n",
    "    criterion = masked_mse_loss\n",
    "    criterion_dab = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "    # Train the model\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_avg_bio = 0.0\n",
    "    best_model = None\n",
    "    define_wandb_metrcis()\n",
    "\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "        train_loader = prepare_dataloader(\n",
    "            train_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        valid_loader = prepare_dataloader(\n",
    "            valid_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        if config.do_train:\n",
    "            train(\n",
    "                model,\n",
    "                loader=train_loader,\n",
    "            )\n",
    "        val_loss, val_mre = evaluate(\n",
    "            model,\n",
    "            loader=valid_loader,\n",
    "        )\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        logger.info(\"-\" * 89)\n",
    "        logger.info(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n",
    "        )\n",
    "        logger.info(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_model_epoch = epoch\n",
    "            logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    # Extract embeddings\n",
    "    best_model.eval()\n",
    "    adata_t = adata_sorted\n",
    "    adata_t = adata_t.copy()\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "    tokenized_all = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "    src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "        cell_embeddings = best_model.encode_batch(\n",
    "            all_gene_ids,\n",
    "            all_values.float(),\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            batch_size=config.batch_size,\n",
    "            batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "            time_step=0,\n",
    "            return_np=True,\n",
    "        )\n",
    "    cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "        cell_embeddings, axis=1, keepdims=True\n",
    "    )\n",
    "    \n",
    "    # Save the embeddings\n",
    "    df = pd.DataFrame(cell_embeddings, index=adata_sorted.obs.index)\n",
    "    file_out = latent_path + \"_scGPT_\" + str(rep) + \".txt\"\n",
    "    df.to_csv(file_out)\n",
    "    \n",
    "    # Clean up\n",
    "    del adata\n",
    "    del adata_sorted\n",
    "    del adata_t\n",
    "    del best_model\n",
    "    del tokenized_all\n",
    "    del all_counts\n",
    "    del model\n",
    "    del tokenized_train\n",
    "    del tokenized_valid\n",
    "    \n",
    "    # End the logger and the run\n",
    "    run.finish()\n",
    "    wandb.finish()\n",
    "    gc.collect()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad3a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define variables\n",
    "dataset_id = \"Human_Pancreas\"\n",
    "adata_path = \"/work/NMF_project/reproducibility/data/Pancreas/Human_Pancreas.h5ad\"\n",
    "hvg_path = \"/work/NMF_project/reproducibility/data/Pancreas/Human_Pancreas.features\"\n",
    "latent_path = \"/work/NMF_project/reproducibility/data/Pancreas/Human_Pancreas\"\n",
    "\n",
    "for rep in range(5):\n",
    "    ## Set hyperparameters\n",
    "    hyperparameter_defaults = dict(\n",
    "        seed=42,\n",
    "        dataset_name=dataset_id, # Dataset name\n",
    "        do_train=True, # Flag to indicate whether to do update model parameters during training\n",
    "        load_model=\"/work/NMF_project/reproducibility/data/scGPT/Model/\", # Path to pre-trained model\n",
    "        GEPC=True,  # Gene expression modelling for cell objective\n",
    "        ecs_thres=0.8,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "        dab_weight=1.0, # DAR objective weight for batch correction\n",
    "        mask_ratio=0.4, # Default mask ratio\n",
    "        epochs=15, # Default number of epochs for fine-tuning\n",
    "        n_bins=51, # Default number of bins for value binning in data pre-processing\n",
    "        lr=1e-4, # Default learning rate for fine-tuning\n",
    "        batch_size=64, # Default batch size for fine-tuning\n",
    "        layer_size=128,\n",
    "        nlayers=4,\n",
    "        nhead=4, # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "        dropout=0.2, # Default dropout rate during model fine-tuning\n",
    "        schedule_ratio=0.9,  # Default rate for learning rate decay\n",
    "        save_eval_interval=5, # Default model evaluation interval\n",
    "        log_interval=100, # Default log interval\n",
    "        fast_transformer=True, # Default setting\n",
    "        pre_norm=False, # Default setting\n",
    "        amp=True,  # # Default setting: Automatic Mixed Precision\n",
    "    )\n",
    "\n",
    "    ## Initialize the run on wandb\n",
    "    run = wandb.init(\n",
    "        config=hyperparameter_defaults,\n",
    "        project=\"scGPT\",\n",
    "        reinit=True,\n",
    "        settings=wandb.Settings(start_method=\"fork\"),\n",
    "    )\n",
    "    config = wandb.config\n",
    "    print(config)\n",
    "    set_seed(config.seed)\n",
    "\n",
    "    # Settings for input and preprocessing\n",
    "    pad_token = \"<pad>\"\n",
    "    special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "    mask_ratio = config.mask_ratio\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = config.n_bins\n",
    "    n_hvg = 1200  # number of highly variable genes\n",
    "    max_seq_len = n_hvg + 1\n",
    "    per_seq_batch_sample = True\n",
    "    DSBN = True  # Domain-spec batchnorm\n",
    "    explicit_zero_prob = True  # whether explicit bernoulli for zeros\n",
    "\n",
    "    # Settings for saving the model\n",
    "    dataset_name = config.dataset_name\n",
    "    save_dir = Path(f\"/work/NMF_project/reproducibility/data/scGPT/FT/dev_{dataset_name}_{rep}/\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"save to {save_dir}\")\n",
    "    logger = scg.logger\n",
    "    scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "\n",
    "    # Load data\n",
    "    adata = sc.read(adata_path)\n",
    "    ori_batch_col = \"batch_label\"\n",
    "    adata.var = adata.var.set_index(\"features\")\n",
    "    data_is_raw = True\n",
    "\n",
    "    # make the batch category column\n",
    "    adata.obs[\"str_batch\"] = adata.obs[ori_batch_col].astype(str)\n",
    "    batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "    adata.obs[\"batch_id\"] = batch_id_labels\n",
    "    adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "    # Define HVGs\n",
    "    f = open(hvg_path, \"r\")\n",
    "    hvg = f.read().splitlines()\n",
    "    f.close()\n",
    "    adata.var['highly_variable'] = [True if g in hvg else False for g in adata.var_names]\n",
    "\n",
    "    # Load the pretrained model\n",
    "    if config.load_model is not None:\n",
    "        model_dir = Path(config.load_model)\n",
    "        model_config_file = model_dir / \"args.json\"\n",
    "        model_file = model_dir / \"best_model.pt\"\n",
    "        vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "        vocab = GeneVocab.from_file(vocab_file)\n",
    "        for s in special_tokens:\n",
    "            if s not in vocab:\n",
    "                vocab.append_token(s)\n",
    "\n",
    "        adata.var[\"id_in_vocab\"] = [\n",
    "            1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "        ]\n",
    "        gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "        logger.info(\n",
    "            f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "            f\"in vocabulary of size {len(vocab)}.\"\n",
    "        )\n",
    "        adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "        # model\n",
    "        with open(model_config_file, \"r\") as f:\n",
    "            model_configs = json.load(f)\n",
    "        logger.info(\n",
    "            f\"Resume model from {model_file}, the model args will be overriden by the \"\n",
    "            f\"config {model_config_file}.\"\n",
    "        )\n",
    "        embsize = model_configs[\"embsize\"]\n",
    "        nhead = model_configs[\"nheads\"]\n",
    "        d_hid = model_configs[\"d_hid\"]\n",
    "        nlayers = model_configs[\"nlayers\"]\n",
    "        n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "    else:\n",
    "        embsize = config.layer_size\n",
    "        nhead = config.nhead\n",
    "        nlayers = config.nlayers\n",
    "        d_hid = config.layer_size\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    sc.pp.filter_genes(adata, min_counts=3)\n",
    "    normed = sc.pp.normalize_total(adata, target_sum=1e4, layer=None, inplace=False)[\"X\"]\n",
    "    sc.get._set_obs_rep(adata, normed, layer=\"X_normed\")\n",
    "    sc.get._set_obs_rep(adata,sc.get._get_obs_rep(adata, layer=\"X_normed\"), layer=\"X_log1p\")\n",
    "    sc.pp.log1p(adata, layer=\"X_log1p\")\n",
    "    adata = adata[:, adata.var.highly_variable]\n",
    "    n_bins = config.n_bins  # NOTE: the first bin is always a spectial for zero\n",
    "    binned_rows = []\n",
    "    bin_edges = []\n",
    "    layer_data = sc.get._get_obs_rep(adata, layer=\"X_log1p\")\n",
    "    layer_data = layer_data.A if issparse(layer_data) else layer_data\n",
    "    for row in layer_data:\n",
    "        non_zero_ids = row.nonzero()\n",
    "        non_zero_row = row[non_zero_ids]\n",
    "        bins = np.quantile(non_zero_row, np.linspace(0, 1, n_bins - 1))\n",
    "        non_zero_digits = _digitize(x = non_zero_row, bins = bins)\n",
    "        assert non_zero_digits.min() >= 1\n",
    "        assert non_zero_digits.max() <= n_bins - 1\n",
    "        binned_row = np.zeros_like(row, dtype=np.int64)\n",
    "        binned_row[non_zero_ids] = non_zero_digits\n",
    "        binned_rows.append(binned_row)\n",
    "        bin_edges.append(np.concatenate([[0], bins]))\n",
    "    adata.layers[\"X_binned\"] = np.stack(binned_rows)\n",
    "    adata.obsm[\"bin_edges\"] = np.stack(bin_edges)\n",
    "\n",
    "    # Sort the adata by batch_id in advance\n",
    "    if per_seq_batch_sample:\n",
    "        adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()\n",
    "\n",
    "    # Define input layers and get counts\n",
    "    input_layer_key = \"X_binned\"\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "    genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "    # Get batch ids\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    num_batch_types = len(set(batch_ids))\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Create splits\n",
    "    (\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        train_batch_labels,\n",
    "        valid_batch_labels,\n",
    "    ) = train_test_split(\n",
    "        all_counts, batch_ids, test_size=0.1, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Define vocabulary\n",
    "    if config.load_model is None:\n",
    "        vocab = Vocab(\n",
    "            VocabPybind(genes + special_tokens, None)\n",
    "        )  # bidirectional lookup [gene <-> int]\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)\n",
    "\n",
    "    # Tokenize training and validation data\n",
    "    tokenized_train = tokenize_and_pad_batch(\n",
    "        train_data,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    tokenized_valid = tokenize_and_pad_batch(\n",
    "        valid_data,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "        f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "        f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    "    )\n",
    "\n",
    "    # Load the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ntokens = len(vocab)  # size of vocabulary\n",
    "    model = TransformerModel(\n",
    "        ntokens,\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid,\n",
    "        nlayers,\n",
    "        vocab=vocab,\n",
    "        dropout=config.dropout,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        do_mvc=config.GEPC,\n",
    "        do_dab=True,\n",
    "        use_batch_labels=True,\n",
    "        num_batch_labels=num_batch_types,\n",
    "        domain_spec_batchnorm=DSBN,\n",
    "        n_input_bins=n_input_bins,\n",
    "        ecs_threshold=config.ecs_thres,\n",
    "        explicit_zero_prob=explicit_zero_prob,\n",
    "        use_fast_transformer=config.fast_transformer,\n",
    "        pre_norm=config.pre_norm,\n",
    "    )\n",
    "    if config.load_model is not None:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_file))\n",
    "            logger.info(f\"Loading all model params from {model_file}\")\n",
    "        except:\n",
    "            # only load params that are in the model and match the size\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = torch.load(model_file)\n",
    "            pretrained_dict = {\n",
    "                k: v\n",
    "                for k, v in pretrained_dict.items()\n",
    "                if k in model_dict and v.shape == model_dict[k].shape\n",
    "            }\n",
    "            for k, v in pretrained_dict.items():\n",
    "                logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    wandb.watch(model)\n",
    "\n",
    "    # Set model criteria\n",
    "    criterion = masked_mse_loss\n",
    "    criterion_dab = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "    # Train the model\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_avg_bio = 0.0\n",
    "    best_model = None\n",
    "    define_wandb_metrcis()\n",
    "\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "        train_loader = prepare_dataloader(\n",
    "            train_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        valid_loader = prepare_dataloader(\n",
    "            valid_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        if config.do_train:\n",
    "            train(\n",
    "                model,\n",
    "                loader=train_loader,\n",
    "            )\n",
    "        val_loss, val_mre = evaluate(\n",
    "            model,\n",
    "            loader=valid_loader,\n",
    "        )\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        logger.info(\"-\" * 89)\n",
    "        logger.info(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n",
    "        )\n",
    "        logger.info(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_model_epoch = epoch\n",
    "            logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    # Extract embeddings\n",
    "    best_model.eval()\n",
    "    adata_t = adata_sorted\n",
    "    adata_t = adata_t.copy()\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "    tokenized_all = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "    src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "        cell_embeddings = best_model.encode_batch(\n",
    "            all_gene_ids,\n",
    "            all_values.float(),\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            batch_size=config.batch_size,\n",
    "            batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "            time_step=0,\n",
    "            return_np=True,\n",
    "        )\n",
    "    cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "        cell_embeddings, axis=1, keepdims=True\n",
    "    )\n",
    "    \n",
    "    # Save the embeddings\n",
    "    df = pd.DataFrame(cell_embeddings, index=adata_sorted.obs.index)\n",
    "    file_out = latent_path + \"_scGPT_\" + str(rep) + \".txt\"\n",
    "    df.to_csv(file_out)\n",
    "    \n",
    "    # Clean up\n",
    "    del adata\n",
    "    del adata_sorted\n",
    "    del adata_t\n",
    "    del best_model\n",
    "    del tokenized_all\n",
    "    del all_counts\n",
    "    del model\n",
    "    del tokenized_train\n",
    "    del tokenized_valid\n",
    "    \n",
    "    # End the logger and the run\n",
    "    run.finish()\n",
    "    wandb.finish()\n",
    "    gc.collect()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define variables\n",
    "dataset_id = \"Human_Kidney\"\n",
    "adata_path = \"/work/NMF_project/reproducibility/data/Kidney/Human_Kidney_sub.h5ad\"\n",
    "hvg_path = \"/work/NMF_project/reproducibility/data/Kidney/Human_Kidney_sub.features\"\n",
    "latent_path = \"/work/NMF_project/reproducibility/data/Kidney/Human_Kidney\"\n",
    "\n",
    "for rep in range(5):\n",
    "    ## Set hyperparameters\n",
    "    hyperparameter_defaults = dict(\n",
    "        seed=42,\n",
    "        dataset_name=dataset_id, # Dataset name\n",
    "        do_train=True, # Flag to indicate whether to do update model parameters during training\n",
    "        load_model=\"/work/NMF_project/reproducibility/data/scGPT/Model/\", # Path to pre-trained model\n",
    "        GEPC=True,  # Gene expression modelling for cell objective\n",
    "        ecs_thres=0.8,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "        dab_weight=1.0, # DAR objective weight for batch correction\n",
    "        mask_ratio=0.4, # Default mask ratio\n",
    "        epochs=15, # Default number of epochs for fine-tuning\n",
    "        n_bins=51, # Default number of bins for value binning in data pre-processing\n",
    "        lr=1e-4, # Default learning rate for fine-tuning\n",
    "        batch_size=64, # Default batch size for fine-tuning\n",
    "        layer_size=128,\n",
    "        nlayers=4,\n",
    "        nhead=4, # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "        dropout=0.2, # Default dropout rate during model fine-tuning\n",
    "        schedule_ratio=0.9,  # Default rate for learning rate decay\n",
    "        save_eval_interval=5, # Default model evaluation interval\n",
    "        log_interval=100, # Default log interval\n",
    "        fast_transformer=True, # Default setting\n",
    "        pre_norm=False, # Default setting\n",
    "        amp=True,  # # Default setting: Automatic Mixed Precision\n",
    "    )\n",
    "\n",
    "    ## Initialize the run on wandb\n",
    "    run = wandb.init(\n",
    "        config=hyperparameter_defaults,\n",
    "        project=\"scGPT\",\n",
    "        reinit=True,\n",
    "        settings=wandb.Settings(start_method=\"fork\"),\n",
    "    )\n",
    "    config = wandb.config\n",
    "    print(config)\n",
    "    set_seed(config.seed)\n",
    "\n",
    "    # Settings for input and preprocessing\n",
    "    pad_token = \"<pad>\"\n",
    "    special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "    mask_ratio = config.mask_ratio\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = config.n_bins\n",
    "    n_hvg = 1200  # number of highly variable genes\n",
    "    max_seq_len = n_hvg + 1\n",
    "    per_seq_batch_sample = True\n",
    "    DSBN = True  # Domain-spec batchnorm\n",
    "    explicit_zero_prob = True  # whether explicit bernoulli for zeros\n",
    "\n",
    "    # Settings for saving the model\n",
    "    dataset_name = config.dataset_name\n",
    "    save_dir = Path(f\"/work/NMF_project/reproducibility/data/scGPT/FT/dev_{dataset_name}_{rep}/\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"save to {save_dir}\")\n",
    "    logger = scg.logger\n",
    "    scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "\n",
    "    # Load data\n",
    "    adata = sc.read(adata_path)\n",
    "    ori_batch_col = \"batch_label\"\n",
    "    adata.var = adata.var.set_index(\"features\")\n",
    "    data_is_raw = True\n",
    "\n",
    "    # make the batch category column\n",
    "    adata.obs[\"str_batch\"] = adata.obs[ori_batch_col].astype(str)\n",
    "    batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "    adata.obs[\"batch_id\"] = batch_id_labels\n",
    "    adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "    # Define HVGs\n",
    "    f = open(hvg_path, \"r\")\n",
    "    hvg = f.read().splitlines()\n",
    "    f.close()\n",
    "    adata.var['highly_variable'] = [True if g in hvg else False for g in adata.var_names]\n",
    "\n",
    "    # Load the pretrained model\n",
    "    if config.load_model is not None:\n",
    "        model_dir = Path(config.load_model)\n",
    "        model_config_file = model_dir / \"args.json\"\n",
    "        model_file = model_dir / \"best_model.pt\"\n",
    "        vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "        vocab = GeneVocab.from_file(vocab_file)\n",
    "        for s in special_tokens:\n",
    "            if s not in vocab:\n",
    "                vocab.append_token(s)\n",
    "\n",
    "        adata.var[\"id_in_vocab\"] = [\n",
    "            1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "        ]\n",
    "        gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "        logger.info(\n",
    "            f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "            f\"in vocabulary of size {len(vocab)}.\"\n",
    "        )\n",
    "        adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "        # model\n",
    "        with open(model_config_file, \"r\") as f:\n",
    "            model_configs = json.load(f)\n",
    "        logger.info(\n",
    "            f\"Resume model from {model_file}, the model args will be overriden by the \"\n",
    "            f\"config {model_config_file}.\"\n",
    "        )\n",
    "        embsize = model_configs[\"embsize\"]\n",
    "        nhead = model_configs[\"nheads\"]\n",
    "        d_hid = model_configs[\"d_hid\"]\n",
    "        nlayers = model_configs[\"nlayers\"]\n",
    "        n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "    else:\n",
    "        embsize = config.layer_size\n",
    "        nhead = config.nhead\n",
    "        nlayers = config.nlayers\n",
    "        d_hid = config.layer_size\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    sc.pp.filter_genes(adata, min_counts=3)\n",
    "    normed = sc.pp.normalize_total(adata, target_sum=1e4, layer=None, inplace=False)[\"X\"]\n",
    "    sc.get._set_obs_rep(adata, normed, layer=\"X_normed\")\n",
    "    sc.get._set_obs_rep(adata,sc.get._get_obs_rep(adata, layer=\"X_normed\"), layer=\"X_log1p\")\n",
    "    sc.pp.log1p(adata, layer=\"X_log1p\")\n",
    "    adata = adata[:, adata.var.highly_variable]\n",
    "    n_bins = config.n_bins  # NOTE: the first bin is always a spectial for zero\n",
    "    binned_rows = []\n",
    "    bin_edges = []\n",
    "    layer_data = sc.get._get_obs_rep(adata, layer=\"X_log1p\")\n",
    "    layer_data = layer_data.A if issparse(layer_data) else layer_data\n",
    "    for row in layer_data:\n",
    "        non_zero_ids = row.nonzero()\n",
    "        non_zero_row = row[non_zero_ids]\n",
    "        bins = np.quantile(non_zero_row, np.linspace(0, 1, n_bins - 1))\n",
    "        non_zero_digits = _digitize(x = non_zero_row, bins = bins)\n",
    "        assert non_zero_digits.min() >= 1\n",
    "        assert non_zero_digits.max() <= n_bins - 1\n",
    "        binned_row = np.zeros_like(row, dtype=np.int64)\n",
    "        binned_row[non_zero_ids] = non_zero_digits\n",
    "        binned_rows.append(binned_row)\n",
    "        bin_edges.append(np.concatenate([[0], bins]))\n",
    "    adata.layers[\"X_binned\"] = np.stack(binned_rows)\n",
    "    adata.obsm[\"bin_edges\"] = np.stack(bin_edges)\n",
    "\n",
    "    # Sort the adata by batch_id in advance\n",
    "    if per_seq_batch_sample:\n",
    "        adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()\n",
    "\n",
    "    # Define input layers and get counts\n",
    "    input_layer_key = \"X_binned\"\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "    genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "    # Get batch ids\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    num_batch_types = len(set(batch_ids))\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Create splits\n",
    "    (\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        train_batch_labels,\n",
    "        valid_batch_labels,\n",
    "    ) = train_test_split(\n",
    "        all_counts, batch_ids, test_size=0.1, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Define vocabulary\n",
    "    if config.load_model is None:\n",
    "        vocab = Vocab(\n",
    "            VocabPybind(genes + special_tokens, None)\n",
    "        )  # bidirectional lookup [gene <-> int]\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)\n",
    "\n",
    "    # Tokenize training and validation data\n",
    "    tokenized_train = tokenize_and_pad_batch(\n",
    "        train_data,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    tokenized_valid = tokenize_and_pad_batch(\n",
    "        valid_data,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "        f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "        f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    "    )\n",
    "\n",
    "    # Load the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ntokens = len(vocab)  # size of vocabulary\n",
    "    model = TransformerModel(\n",
    "        ntokens,\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid,\n",
    "        nlayers,\n",
    "        vocab=vocab,\n",
    "        dropout=config.dropout,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        do_mvc=config.GEPC,\n",
    "        do_dab=True,\n",
    "        use_batch_labels=True,\n",
    "        num_batch_labels=num_batch_types,\n",
    "        domain_spec_batchnorm=DSBN,\n",
    "        n_input_bins=n_input_bins,\n",
    "        ecs_threshold=config.ecs_thres,\n",
    "        explicit_zero_prob=explicit_zero_prob,\n",
    "        use_fast_transformer=config.fast_transformer,\n",
    "        pre_norm=config.pre_norm,\n",
    "    )\n",
    "    if config.load_model is not None:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_file))\n",
    "            logger.info(f\"Loading all model params from {model_file}\")\n",
    "        except:\n",
    "            # only load params that are in the model and match the size\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = torch.load(model_file)\n",
    "            pretrained_dict = {\n",
    "                k: v\n",
    "                for k, v in pretrained_dict.items()\n",
    "                if k in model_dict and v.shape == model_dict[k].shape\n",
    "            }\n",
    "            for k, v in pretrained_dict.items():\n",
    "                logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    wandb.watch(model)\n",
    "\n",
    "    # Set model criteria\n",
    "    criterion = masked_mse_loss\n",
    "    criterion_dab = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "    # Train the model\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_avg_bio = 0.0\n",
    "    best_model = None\n",
    "    define_wandb_metrcis()\n",
    "\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "        train_loader = prepare_dataloader(\n",
    "            train_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        valid_loader = prepare_dataloader(\n",
    "            valid_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        if config.do_train:\n",
    "            train(\n",
    "                model,\n",
    "                loader=train_loader,\n",
    "            )\n",
    "        val_loss, val_mre = evaluate(\n",
    "            model,\n",
    "            loader=valid_loader,\n",
    "        )\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        logger.info(\"-\" * 89)\n",
    "        logger.info(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n",
    "        )\n",
    "        logger.info(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_model_epoch = epoch\n",
    "            logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    # Extract embeddings\n",
    "    best_model.eval()\n",
    "    adata_t = adata_sorted\n",
    "    adata_t = adata_t.copy()\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "    tokenized_all = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "    src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "        cell_embeddings = best_model.encode_batch(\n",
    "            all_gene_ids,\n",
    "            all_values.float(),\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            batch_size=config.batch_size,\n",
    "            batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "            time_step=0,\n",
    "            return_np=True,\n",
    "        )\n",
    "    cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "        cell_embeddings, axis=1, keepdims=True\n",
    "    )\n",
    "    \n",
    "    # Save the embeddings\n",
    "    df = pd.DataFrame(cell_embeddings, index=adata_sorted.obs.index)\n",
    "    file_out = latent_path + \"_scGPT_\" + str(rep) + \".txt\"\n",
    "    df.to_csv(file_out)\n",
    "    \n",
    "    # Clean up\n",
    "    del adata\n",
    "    del adata_sorted\n",
    "    del adata_t\n",
    "    del best_model\n",
    "    del tokenized_all\n",
    "    del all_counts\n",
    "    del model\n",
    "    del tokenized_train\n",
    "    del tokenized_valid\n",
    "    \n",
    "    # End the logger and the run\n",
    "    run.finish()\n",
    "    wandb.finish()\n",
    "    gc.collect()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f19fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define variables\n",
    "dataset_id = \"PBMC\"\n",
    "adata_path = \"/work/NMF_project/reproducibility/data/PBMC/PBMC.h5ad\"\n",
    "hvg_path = \"/work/NMF_project/reproducibility/data/PBMC/PBMC.features\"\n",
    "latent_path = \"/work/NMF_project/reproducibility/data/PBMC/PBMC\"\n",
    "\n",
    "for rep in range(5):\n",
    "    ## Set hyperparameters\n",
    "    hyperparameter_defaults = dict(\n",
    "        seed=42,\n",
    "        dataset_name=dataset_id, # Dataset name\n",
    "        do_train=True, # Flag to indicate whether to do update model parameters during training\n",
    "        load_model=\"/work/NMF_project/reproducibility/data/scGPT/Model/\", # Path to pre-trained model\n",
    "        GEPC=True,  # Gene expression modelling for cell objective\n",
    "        ecs_thres=0.8,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "        dab_weight=1.0, # DAR objective weight for batch correction\n",
    "        mask_ratio=0.4, # Default mask ratio\n",
    "        epochs=15, # Default number of epochs for fine-tuning\n",
    "        n_bins=51, # Default number of bins for value binning in data pre-processing\n",
    "        lr=1e-4, # Default learning rate for fine-tuning\n",
    "        batch_size=64, # Default batch size for fine-tuning\n",
    "        layer_size=128,\n",
    "        nlayers=4,\n",
    "        nhead=4, # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "        dropout=0.2, # Default dropout rate during model fine-tuning\n",
    "        schedule_ratio=0.9,  # Default rate for learning rate decay\n",
    "        save_eval_interval=5, # Default model evaluation interval\n",
    "        log_interval=100, # Default log interval\n",
    "        fast_transformer=True, # Default setting\n",
    "        pre_norm=False, # Default setting\n",
    "        amp=True,  # # Default setting: Automatic Mixed Precision\n",
    "    )\n",
    "\n",
    "    ## Initialize the run on wandb\n",
    "    run = wandb.init(\n",
    "        config=hyperparameter_defaults,\n",
    "        project=\"scGPT\",\n",
    "        reinit=True,\n",
    "        settings=wandb.Settings(start_method=\"fork\"),\n",
    "    )\n",
    "    config = wandb.config\n",
    "    print(config)\n",
    "    set_seed(config.seed)\n",
    "\n",
    "    # Settings for input and preprocessing\n",
    "    pad_token = \"<pad>\"\n",
    "    special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "    mask_ratio = config.mask_ratio\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = config.n_bins\n",
    "    n_hvg = 1200  # number of highly variable genes\n",
    "    max_seq_len = n_hvg + 1\n",
    "    per_seq_batch_sample = True\n",
    "    DSBN = True  # Domain-spec batchnorm\n",
    "    explicit_zero_prob = True  # whether explicit bernoulli for zeros\n",
    "\n",
    "    # Settings for saving the model\n",
    "    dataset_name = config.dataset_name\n",
    "    save_dir = Path(f\"/work/NMF_project/reproducibility/data/scGPT/FT/dev_{dataset_name}_{rep}/\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"save to {save_dir}\")\n",
    "    logger = scg.logger\n",
    "    scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "\n",
    "    # Load data\n",
    "    adata = sc.read(adata_path)\n",
    "    ori_batch_col = \"batch_label\"\n",
    "    adata.var = adata.var.set_index(\"features\")\n",
    "    data_is_raw = True\n",
    "\n",
    "    # make the batch category column\n",
    "    adata.obs[\"str_batch\"] = adata.obs[ori_batch_col].astype(str)\n",
    "    batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "    adata.obs[\"batch_id\"] = batch_id_labels\n",
    "    adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "    # Define HVGs\n",
    "    f = open(hvg_path, \"r\")\n",
    "    hvg = f.read().splitlines()\n",
    "    f.close()\n",
    "    adata.var['highly_variable'] = [True if g in hvg else False for g in adata.var_names]\n",
    "\n",
    "    # Load the pretrained model\n",
    "    if config.load_model is not None:\n",
    "        model_dir = Path(config.load_model)\n",
    "        model_config_file = model_dir / \"args.json\"\n",
    "        model_file = model_dir / \"best_model.pt\"\n",
    "        vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "        vocab = GeneVocab.from_file(vocab_file)\n",
    "        for s in special_tokens:\n",
    "            if s not in vocab:\n",
    "                vocab.append_token(s)\n",
    "\n",
    "        adata.var[\"id_in_vocab\"] = [\n",
    "            1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "        ]\n",
    "        gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "        logger.info(\n",
    "            f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "            f\"in vocabulary of size {len(vocab)}.\"\n",
    "        )\n",
    "        adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "        # model\n",
    "        with open(model_config_file, \"r\") as f:\n",
    "            model_configs = json.load(f)\n",
    "        logger.info(\n",
    "            f\"Resume model from {model_file}, the model args will be overriden by the \"\n",
    "            f\"config {model_config_file}.\"\n",
    "        )\n",
    "        embsize = model_configs[\"embsize\"]\n",
    "        nhead = model_configs[\"nheads\"]\n",
    "        d_hid = model_configs[\"d_hid\"]\n",
    "        nlayers = model_configs[\"nlayers\"]\n",
    "        n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "    else:\n",
    "        embsize = config.layer_size\n",
    "        nhead = config.nhead\n",
    "        nlayers = config.nlayers\n",
    "        d_hid = config.layer_size\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    sc.pp.filter_genes(adata, min_counts=3)\n",
    "    normed = sc.pp.normalize_total(adata, target_sum=1e4, layer=None, inplace=False)[\"X\"]\n",
    "    sc.get._set_obs_rep(adata, normed, layer=\"X_normed\")\n",
    "    sc.get._set_obs_rep(adata,sc.get._get_obs_rep(adata, layer=\"X_normed\"), layer=\"X_log1p\")\n",
    "    sc.pp.log1p(adata, layer=\"X_log1p\")\n",
    "    adata = adata[:, adata.var.highly_variable]\n",
    "    n_bins = config.n_bins  # NOTE: the first bin is always a spectial for zero\n",
    "    binned_rows = []\n",
    "    bin_edges = []\n",
    "    layer_data = sc.get._get_obs_rep(adata, layer=\"X_log1p\")\n",
    "    layer_data = layer_data.A if issparse(layer_data) else layer_data\n",
    "    for row in layer_data:\n",
    "        non_zero_ids = row.nonzero()\n",
    "        non_zero_row = row[non_zero_ids]\n",
    "        bins = np.quantile(non_zero_row, np.linspace(0, 1, n_bins - 1))\n",
    "        non_zero_digits = _digitize(x = non_zero_row, bins = bins)\n",
    "        assert non_zero_digits.min() >= 1\n",
    "        assert non_zero_digits.max() <= n_bins - 1\n",
    "        binned_row = np.zeros_like(row, dtype=np.int64)\n",
    "        binned_row[non_zero_ids] = non_zero_digits\n",
    "        binned_rows.append(binned_row)\n",
    "        bin_edges.append(np.concatenate([[0], bins]))\n",
    "    adata.layers[\"X_binned\"] = np.stack(binned_rows)\n",
    "    adata.obsm[\"bin_edges\"] = np.stack(bin_edges)\n",
    "\n",
    "    # Sort the adata by batch_id in advance\n",
    "    if per_seq_batch_sample:\n",
    "        adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()\n",
    "\n",
    "    # Define input layers and get counts\n",
    "    input_layer_key = \"X_binned\"\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "    genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "    # Get batch ids\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    num_batch_types = len(set(batch_ids))\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Create splits\n",
    "    (\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        train_batch_labels,\n",
    "        valid_batch_labels,\n",
    "    ) = train_test_split(\n",
    "        all_counts, batch_ids, test_size=0.1, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Define vocabulary\n",
    "    if config.load_model is None:\n",
    "        vocab = Vocab(\n",
    "            VocabPybind(genes + special_tokens, None)\n",
    "        )  # bidirectional lookup [gene <-> int]\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)\n",
    "\n",
    "    # Tokenize training and validation data\n",
    "    tokenized_train = tokenize_and_pad_batch(\n",
    "        train_data,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    tokenized_valid = tokenize_and_pad_batch(\n",
    "        valid_data,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "        f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "        f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    "    )\n",
    "\n",
    "    # Load the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ntokens = len(vocab)  # size of vocabulary\n",
    "    model = TransformerModel(\n",
    "        ntokens,\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid,\n",
    "        nlayers,\n",
    "        vocab=vocab,\n",
    "        dropout=config.dropout,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        do_mvc=config.GEPC,\n",
    "        do_dab=True,\n",
    "        use_batch_labels=True,\n",
    "        num_batch_labels=num_batch_types,\n",
    "        domain_spec_batchnorm=DSBN,\n",
    "        n_input_bins=n_input_bins,\n",
    "        ecs_threshold=config.ecs_thres,\n",
    "        explicit_zero_prob=explicit_zero_prob,\n",
    "        use_fast_transformer=config.fast_transformer,\n",
    "        pre_norm=config.pre_norm,\n",
    "    )\n",
    "    if config.load_model is not None:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_file))\n",
    "            logger.info(f\"Loading all model params from {model_file}\")\n",
    "        except:\n",
    "            # only load params that are in the model and match the size\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = torch.load(model_file)\n",
    "            pretrained_dict = {\n",
    "                k: v\n",
    "                for k, v in pretrained_dict.items()\n",
    "                if k in model_dict and v.shape == model_dict[k].shape\n",
    "            }\n",
    "            for k, v in pretrained_dict.items():\n",
    "                logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    wandb.watch(model)\n",
    "\n",
    "    # Set model criteria\n",
    "    criterion = masked_mse_loss\n",
    "    criterion_dab = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "    # Train the model\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_avg_bio = 0.0\n",
    "    best_model = None\n",
    "    define_wandb_metrcis()\n",
    "\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "        train_loader = prepare_dataloader(\n",
    "            train_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        valid_loader = prepare_dataloader(\n",
    "            valid_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        if config.do_train:\n",
    "            train(\n",
    "                model,\n",
    "                loader=train_loader,\n",
    "            )\n",
    "        val_loss, val_mre = evaluate(\n",
    "            model,\n",
    "            loader=valid_loader,\n",
    "        )\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        logger.info(\"-\" * 89)\n",
    "        logger.info(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n",
    "        )\n",
    "        logger.info(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_model_epoch = epoch\n",
    "            logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    # Extract embeddings\n",
    "    best_model.eval()\n",
    "    adata_t = adata_sorted\n",
    "    adata_t = adata_t.copy()\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "    tokenized_all = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "    src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "        cell_embeddings = best_model.encode_batch(\n",
    "            all_gene_ids,\n",
    "            all_values.float(),\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            batch_size=config.batch_size,\n",
    "            batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "            time_step=0,\n",
    "            return_np=True,\n",
    "        )\n",
    "    cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "        cell_embeddings, axis=1, keepdims=True\n",
    "    )\n",
    "    \n",
    "    # Save the embeddings\n",
    "    df = pd.DataFrame(cell_embeddings, index=adata_sorted.obs.index)\n",
    "    file_out = latent_path + \"_scGPT_\" + str(rep) + \".txt\"\n",
    "    df.to_csv(file_out)\n",
    "    \n",
    "    # Clean up\n",
    "    del adata\n",
    "    del adata_sorted\n",
    "    del adata_t\n",
    "    del best_model\n",
    "    del tokenized_all\n",
    "    del all_counts\n",
    "    del model\n",
    "    del tokenized_train\n",
    "    del tokenized_valid\n",
    "    \n",
    "    # End the logger and the run\n",
    "    run.finish()\n",
    "    wandb.finish()\n",
    "    gc.collect()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9501fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define variables\n",
    "dataset_id = \"Human_Lung\"\n",
    "adata_path = \"/work/NMF_project/reproducibility/data/Lung/Human_Lung_sub.h5ad\"\n",
    "hvg_path = \"/work/NMF_project/reproducibility/data/Lung/Human_Lung_sub.features\"\n",
    "latent_path = \"/work/NMF_project/reproducibility/data/Lung/Human_Lung\"\n",
    "\n",
    "for rep in range(5):\n",
    "    ## Set hyperparameters\n",
    "    hyperparameter_defaults = dict(\n",
    "        seed=42,\n",
    "        dataset_name=dataset_id, # Dataset name\n",
    "        do_train=True, # Flag to indicate whether to do update model parameters during training\n",
    "        load_model=\"/work/NMF_project/reproducibility/data/scGPT/Model/\", # Path to pre-trained model\n",
    "        GEPC=True,  # Gene expression modelling for cell objective\n",
    "        ecs_thres=0.8,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "        dab_weight=1.0, # DAR objective weight for batch correction\n",
    "        mask_ratio=0.4, # Default mask ratio\n",
    "        epochs=15, # Default number of epochs for fine-tuning\n",
    "        n_bins=51, # Default number of bins for value binning in data pre-processing\n",
    "        lr=1e-4, # Default learning rate for fine-tuning\n",
    "        batch_size=64, # Default batch size for fine-tuning\n",
    "        layer_size=128,\n",
    "        nlayers=4,\n",
    "        nhead=4, # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "        dropout=0.2, # Default dropout rate during model fine-tuning\n",
    "        schedule_ratio=0.9,  # Default rate for learning rate decay\n",
    "        save_eval_interval=5, # Default model evaluation interval\n",
    "        log_interval=100, # Default log interval\n",
    "        fast_transformer=True, # Default setting\n",
    "        pre_norm=False, # Default setting\n",
    "        amp=True,  # # Default setting: Automatic Mixed Precision\n",
    "    )\n",
    "\n",
    "    ## Initialize the run on wandb\n",
    "    run = wandb.init(\n",
    "        config=hyperparameter_defaults,\n",
    "        project=\"scGPT\",\n",
    "        reinit=True,\n",
    "        settings=wandb.Settings(start_method=\"fork\"),\n",
    "    )\n",
    "    config = wandb.config\n",
    "    print(config)\n",
    "    set_seed(config.seed)\n",
    "\n",
    "    # Settings for input and preprocessing\n",
    "    pad_token = \"<pad>\"\n",
    "    special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "    mask_ratio = config.mask_ratio\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = config.n_bins\n",
    "    n_hvg = 1200  # number of highly variable genes\n",
    "    max_seq_len = n_hvg + 1\n",
    "    per_seq_batch_sample = True\n",
    "    DSBN = True  # Domain-spec batchnorm\n",
    "    explicit_zero_prob = True  # whether explicit bernoulli for zeros\n",
    "\n",
    "    # Settings for saving the model\n",
    "    dataset_name = config.dataset_name\n",
    "    save_dir = Path(f\"/work/NMF_project/reproducibility/data/scGPT/FT/dev_{dataset_name}_{rep}/\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"save to {save_dir}\")\n",
    "    logger = scg.logger\n",
    "    scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "\n",
    "    # Load data\n",
    "    adata = sc.read(adata_path)\n",
    "    ori_batch_col = \"batch_label\"\n",
    "    adata.var = adata.var.set_index(\"features\")\n",
    "    data_is_raw = True\n",
    "\n",
    "    # make the batch category column\n",
    "    adata.obs[\"str_batch\"] = adata.obs[ori_batch_col].astype(str)\n",
    "    batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "    adata.obs[\"batch_id\"] = batch_id_labels\n",
    "    adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "    # Define HVGs\n",
    "    f = open(hvg_path, \"r\")\n",
    "    hvg = f.read().splitlines()\n",
    "    f.close()\n",
    "    adata.var['highly_variable'] = [True if g in hvg else False for g in adata.var_names]\n",
    "\n",
    "    # Load the pretrained model\n",
    "    if config.load_model is not None:\n",
    "        model_dir = Path(config.load_model)\n",
    "        model_config_file = model_dir / \"args.json\"\n",
    "        model_file = model_dir / \"best_model.pt\"\n",
    "        vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "        vocab = GeneVocab.from_file(vocab_file)\n",
    "        for s in special_tokens:\n",
    "            if s not in vocab:\n",
    "                vocab.append_token(s)\n",
    "\n",
    "        adata.var[\"id_in_vocab\"] = [\n",
    "            1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "        ]\n",
    "        gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "        logger.info(\n",
    "            f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "            f\"in vocabulary of size {len(vocab)}.\"\n",
    "        )\n",
    "        adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "        # model\n",
    "        with open(model_config_file, \"r\") as f:\n",
    "            model_configs = json.load(f)\n",
    "        logger.info(\n",
    "            f\"Resume model from {model_file}, the model args will be overriden by the \"\n",
    "            f\"config {model_config_file}.\"\n",
    "        )\n",
    "        embsize = model_configs[\"embsize\"]\n",
    "        nhead = model_configs[\"nheads\"]\n",
    "        d_hid = model_configs[\"d_hid\"]\n",
    "        nlayers = model_configs[\"nlayers\"]\n",
    "        n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "    else:\n",
    "        embsize = config.layer_size\n",
    "        nhead = config.nhead\n",
    "        nlayers = config.nlayers\n",
    "        d_hid = config.layer_size\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    sc.pp.filter_genes(adata, min_counts=3)\n",
    "    normed = sc.pp.normalize_total(adata, target_sum=1e4, layer=None, inplace=False)[\"X\"]\n",
    "    sc.get._set_obs_rep(adata, normed, layer=\"X_normed\")\n",
    "    sc.get._set_obs_rep(adata,sc.get._get_obs_rep(adata, layer=\"X_normed\"), layer=\"X_log1p\")\n",
    "    sc.pp.log1p(adata, layer=\"X_log1p\")\n",
    "    adata = adata[:, adata.var.highly_variable]\n",
    "    n_bins = config.n_bins  # NOTE: the first bin is always a spectial for zero\n",
    "    binned_rows = []\n",
    "    bin_edges = []\n",
    "    layer_data = sc.get._get_obs_rep(adata, layer=\"X_log1p\")\n",
    "    layer_data = layer_data.A if issparse(layer_data) else layer_data\n",
    "    for row in layer_data:\n",
    "        non_zero_ids = row.nonzero()\n",
    "        non_zero_row = row[non_zero_ids]\n",
    "        bins = np.quantile(non_zero_row, np.linspace(0, 1, n_bins - 1))\n",
    "        non_zero_digits = _digitize(x = non_zero_row, bins = bins)\n",
    "        assert non_zero_digits.min() >= 1\n",
    "        assert non_zero_digits.max() <= n_bins - 1\n",
    "        binned_row = np.zeros_like(row, dtype=np.int64)\n",
    "        binned_row[non_zero_ids] = non_zero_digits\n",
    "        binned_rows.append(binned_row)\n",
    "        bin_edges.append(np.concatenate([[0], bins]))\n",
    "    adata.layers[\"X_binned\"] = np.stack(binned_rows)\n",
    "    adata.obsm[\"bin_edges\"] = np.stack(bin_edges)\n",
    "\n",
    "    # Sort the adata by batch_id in advance\n",
    "    if per_seq_batch_sample:\n",
    "        adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()\n",
    "\n",
    "    # Define input layers and get counts\n",
    "    input_layer_key = \"X_binned\"\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "    genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "    # Get batch ids\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    num_batch_types = len(set(batch_ids))\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Create splits\n",
    "    (\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        train_batch_labels,\n",
    "        valid_batch_labels,\n",
    "    ) = train_test_split(\n",
    "        all_counts, batch_ids, test_size=0.1, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Define vocabulary\n",
    "    if config.load_model is None:\n",
    "        vocab = Vocab(\n",
    "            VocabPybind(genes + special_tokens, None)\n",
    "        )  # bidirectional lookup [gene <-> int]\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)\n",
    "\n",
    "    # Tokenize training and validation data\n",
    "    tokenized_train = tokenize_and_pad_batch(\n",
    "        train_data,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    tokenized_valid = tokenize_and_pad_batch(\n",
    "        valid_data,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "        f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "        f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    "    )\n",
    "\n",
    "    # Load the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ntokens = len(vocab)  # size of vocabulary\n",
    "    model = TransformerModel(\n",
    "        ntokens,\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid,\n",
    "        nlayers,\n",
    "        vocab=vocab,\n",
    "        dropout=config.dropout,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        do_mvc=config.GEPC,\n",
    "        do_dab=True,\n",
    "        use_batch_labels=True,\n",
    "        num_batch_labels=num_batch_types,\n",
    "        domain_spec_batchnorm=DSBN,\n",
    "        n_input_bins=n_input_bins,\n",
    "        ecs_threshold=config.ecs_thres,\n",
    "        explicit_zero_prob=explicit_zero_prob,\n",
    "        use_fast_transformer=config.fast_transformer,\n",
    "        pre_norm=config.pre_norm,\n",
    "    )\n",
    "    if config.load_model is not None:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_file))\n",
    "            logger.info(f\"Loading all model params from {model_file}\")\n",
    "        except:\n",
    "            # only load params that are in the model and match the size\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = torch.load(model_file)\n",
    "            pretrained_dict = {\n",
    "                k: v\n",
    "                for k, v in pretrained_dict.items()\n",
    "                if k in model_dict and v.shape == model_dict[k].shape\n",
    "            }\n",
    "            for k, v in pretrained_dict.items():\n",
    "                logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    wandb.watch(model)\n",
    "\n",
    "    # Set model criteria\n",
    "    criterion = masked_mse_loss\n",
    "    criterion_dab = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "    # Train the model\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_avg_bio = 0.0\n",
    "    best_model = None\n",
    "    define_wandb_metrcis()\n",
    "\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "        train_loader = prepare_dataloader(\n",
    "            train_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        valid_loader = prepare_dataloader(\n",
    "            valid_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        if config.do_train:\n",
    "            train(\n",
    "                model,\n",
    "                loader=train_loader,\n",
    "            )\n",
    "        val_loss, val_mre = evaluate(\n",
    "            model,\n",
    "            loader=valid_loader,\n",
    "        )\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        logger.info(\"-\" * 89)\n",
    "        logger.info(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n",
    "        )\n",
    "        logger.info(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_model_epoch = epoch\n",
    "            logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    # Extract embeddings\n",
    "    best_model.eval()\n",
    "    adata_t = adata_sorted\n",
    "    adata_t = adata_t.copy()\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "    tokenized_all = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "    src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "        cell_embeddings = best_model.encode_batch(\n",
    "            all_gene_ids,\n",
    "            all_values.float(),\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            batch_size=config.batch_size,\n",
    "            batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "            time_step=0,\n",
    "            return_np=True,\n",
    "        )\n",
    "    cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "        cell_embeddings, axis=1, keepdims=True\n",
    "    )\n",
    "    \n",
    "    # Save the embeddings\n",
    "    df = pd.DataFrame(cell_embeddings, index=adata_sorted.obs.index)\n",
    "    file_out = latent_path + \"_scGPT_\" + str(rep) + \".txt\"\n",
    "    df.to_csv(file_out)\n",
    "    \n",
    "    # Clean up\n",
    "    del adata\n",
    "    del adata_sorted\n",
    "    del adata_t\n",
    "    del best_model\n",
    "    del tokenized_all\n",
    "    del all_counts\n",
    "    del model\n",
    "    del tokenized_train\n",
    "    del tokenized_valid\n",
    "    \n",
    "    # End the logger and the run\n",
    "    run.finish()\n",
    "    wandb.finish()\n",
    "    gc.collect()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10496a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjespergrud\u001b[0m (\u001b[33mmadlab_sdu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/NMF_project/reproducibility/wandb/run-20230829_120037-2ajgbs6k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/madlab_sdu/scGPT/runs/2ajgbs6k\" target=\"_blank\">graceful-sun-47</a></strong> to <a href=\"https://wandb.ai/madlab_sdu/scGPT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'Mixture', 'do_train': True, 'load_model': '/work/NMF_project/reproducibility/data/scGPT/Model/', 'GEPC': True, 'ecs_thres': 0.8, 'dab_weight': 1.0, 'mask_ratio': 0.4, 'epochs': 15, 'n_bins': 51, 'lr': 0.0001, 'batch_size': 64, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True}\n",
      "save to /work/NMF_project/reproducibility/data/scGPT/FT/dev_Mixture_0\n",
      "scGPT - INFO - match 13123/13760 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /work/NMF_project/reproducibility/data/scGPT/Model/best_model.pt, the model args will be overriden by the config /work/NMF_project/reproducibility/data/scGPT/Model/args.json.\n",
      "scGPT - INFO - train set number of samples: 27747, \n",
      "\t feature length: 1201\n",
      "scGPT - INFO - valid set number of samples: 3083, \n",
      "\t feature length: 1201\n",
      "Use domain specific batchnorm with affine=False\n",
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.bias with shape torch.Size([512])\n",
      "random masking at epoch   1, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   1 | 100/442 batches | lr 0.0001 | ms/batch 305.91 | loss 500.27 | mse 249.36 | mre 5233953.75 |gepc 236.45 |\n",
      "scGPT - INFO - | epoch   1 | 200/442 batches | lr 0.0001 | ms/batch 304.97 | loss 265.32 | mse 117.95 | mre 3307139.65 |gepc 133.50 |\n",
      "scGPT - INFO - | epoch   1 | 300/442 batches | lr 0.0001 | ms/batch 293.75 | loss 192.16 | mse 84.93 | mre 2539657.90 |gepc 92.76 |\n",
      "scGPT - INFO - | epoch   1 | 400/442 batches | lr 0.0001 | ms/batch 297.94 | loss 200.72 | mse 88.27 | mre 2507227.83 |gepc 97.48 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 137.65s | valid loss/mse 118.7551 | mre 3506724.7516\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 118.7551\n",
      "random masking at epoch   2, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   2 | 100/442 batches | lr 0.0001 | ms/batch 299.07 | loss 331.04 | mse 143.30 | mre 3689402.09 |gepc 172.55 |\n",
      "scGPT - INFO - | epoch   2 | 200/442 batches | lr 0.0001 | ms/batch 307.43 | loss 228.21 | mse 103.08 | mre 2880530.10 |gepc 112.23 |\n",
      "scGPT - INFO - | epoch   2 | 300/442 batches | lr 0.0001 | ms/batch 298.62 | loss 179.69 | mse 79.93 | mre 2310314.78 |gepc 85.59 |\n",
      "scGPT - INFO - | epoch   2 | 400/442 batches | lr 0.0001 | ms/batch 302.20 | loss 195.53 | mse 84.42 | mre 2453800.74 |gepc 96.64 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 138.17s | valid loss/mse 106.6372 | mre 2874273.6590\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 106.6372\n",
      "random masking at epoch   3, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   3 | 100/442 batches | lr 0.0001 | ms/batch 300.95 | loss 296.95 | mse 135.56 | mre 3499460.63 |gepc 148.53 |\n",
      "scGPT - INFO - | epoch   3 | 200/442 batches | lr 0.0001 | ms/batch 309.23 | loss 222.19 | mse 100.86 | mre 2821200.65 |gepc 108.55 |\n",
      "scGPT - INFO - | epoch   3 | 300/442 batches | lr 0.0001 | ms/batch 307.61 | loss 185.59 | mse 78.43 | mre 2277729.77 |gepc 93.62 |\n",
      "scGPT - INFO - | epoch   3 | 400/442 batches | lr 0.0001 | ms/batch 308.67 | loss 211.68 | mse 83.56 | mre 2443914.94 |gepc 113.09 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 140.84s | valid loss/mse 102.4873 | mre 2129564.6833\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 102.4873\n",
      "random masking at epoch   4, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   4 | 100/442 batches | lr 0.0001 | ms/batch 316.44 | loss 285.73 | mse 133.12 | mre 3376080.93 |gepc 140.54 |\n",
      "scGPT - INFO - | epoch   4 | 200/442 batches | lr 0.0001 | ms/batch 308.64 | loss 213.12 | mse 98.70 | mre 2746710.49 |gepc 102.75 |\n",
      "scGPT - INFO - | epoch   4 | 300/442 batches | lr 0.0001 | ms/batch 302.77 | loss 179.98 | mse 77.81 | mre 2303891.10 |gepc 89.54 |\n",
      "scGPT - INFO - | epoch   4 | 400/442 batches | lr 0.0001 | ms/batch 308.99 | loss 209.20 | mse 82.72 | mre 2282174.75 |gepc 113.40 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 141.51s | valid loss/mse 99.2422 | mre 2448095.7323\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 99.2422\n",
      "random masking at epoch   5, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   5 | 100/442 batches | lr 0.0001 | ms/batch 311.04 | loss 284.82 | mse 131.28 | mre 3400049.28 |gepc 141.77 |\n",
      "scGPT - INFO - | epoch   5 | 200/442 batches | lr 0.0001 | ms/batch 313.31 | loss 216.06 | mse 98.40 | mre 2726933.32 |gepc 106.66 |\n",
      "scGPT - INFO - | epoch   5 | 300/442 batches | lr 0.0001 | ms/batch 307.90 | loss 170.26 | mse 76.97 | mre 2233185.98 |gepc 81.43 |\n",
      "scGPT - INFO - | epoch   5 | 400/442 batches | lr 0.0001 | ms/batch 308.19 | loss 212.99 | mse 82.09 | mre 2385979.10 |gepc 118.18 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 141.93s | valid loss/mse 98.7309 | mre 2606754.8752\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 98.7309\n",
      "random masking at epoch   6, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   6 | 100/442 batches | lr 0.0001 | ms/batch 307.26 | loss 277.86 | mse 131.03 | mre 3422676.98 |gepc 136.17 |\n",
      "scGPT - INFO - | epoch   6 | 200/442 batches | lr 0.0001 | ms/batch 311.42 | loss 210.47 | mse 97.98 | mre 2730464.92 |gepc 102.19 |\n",
      "scGPT - INFO - | epoch   6 | 300/442 batches | lr 0.0001 | ms/batch 304.39 | loss 171.07 | mse 77.15 | mre 2248315.91 |gepc 81.76 |\n",
      "scGPT - INFO - | epoch   6 | 400/442 batches | lr 0.0001 | ms/batch 306.33 | loss 176.60 | mse 80.74 | mre 2240895.97 |gepc 83.38 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 140.61s | valid loss/mse 96.3023 | mre 2671864.2591\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 96.3023\n",
      "random masking at epoch   7, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   7 | 100/442 batches | lr 0.0001 | ms/batch 309.69 | loss 275.39 | mse 129.79 | mre 3376204.14 |gepc 135.13 |\n",
      "scGPT - INFO - | epoch   7 | 200/442 batches | lr 0.0001 | ms/batch 307.89 | loss 206.43 | mse 96.97 | mre 2724473.34 |gepc 99.39 |\n",
      "scGPT - INFO - | epoch   7 | 300/442 batches | lr 0.0001 | ms/batch 301.89 | loss 170.70 | mse 75.81 | mre 2180112.48 |gepc 83.05 |\n",
      "scGPT - INFO - | epoch   7 | 400/442 batches | lr 0.0001 | ms/batch 308.18 | loss 177.99 | mse 81.04 | mre 2312427.60 |gepc 84.41 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 140.73s | valid loss/mse 95.0787 | mre 2341487.0270\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 95.0787\n",
      "random masking at epoch   8, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   8 | 100/442 batches | lr 0.0000 | ms/batch 309.95 | loss 271.73 | mse 129.12 | mre 3325206.54 |gepc 132.42 |\n",
      "scGPT - INFO - | epoch   8 | 200/442 batches | lr 0.0000 | ms/batch 315.96 | loss 206.99 | mse 96.99 | mre 2763436.88 |gepc 99.98 |\n",
      "scGPT - INFO - | epoch   8 | 300/442 batches | lr 0.0000 | ms/batch 303.24 | loss 164.22 | mse 75.63 | mre 2192438.77 |gepc 76.87 |\n",
      "scGPT - INFO - | epoch   8 | 400/442 batches | lr 0.0000 | ms/batch 305.59 | loss 186.12 | mse 79.61 | mre 2204048.47 |gepc 94.23 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 141.24s | valid loss/mse 94.4049 | mre 2418740.2116\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 94.4049\n",
      "random masking at epoch   9, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   9 | 100/442 batches | lr 0.0000 | ms/batch 308.01 | loss 270.49 | mse 128.61 | mre 3354912.57 |gepc 132.28 |\n",
      "scGPT - INFO - | epoch   9 | 200/442 batches | lr 0.0000 | ms/batch 309.27 | loss 205.78 | mse 96.56 | mre 2682471.43 |gepc 99.32 |\n",
      "scGPT - INFO - | epoch   9 | 300/442 batches | lr 0.0000 | ms/batch 302.05 | loss 164.02 | mse 74.95 | mre 2149733.44 |gepc 77.59 |\n",
      "scGPT - INFO - | epoch   9 | 400/442 batches | lr 0.0000 | ms/batch 305.97 | loss 173.83 | mse 79.29 | mre 2237781.87 |gepc 82.49 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 140.24s | valid loss/mse 95.8842 | mre 2834415.8893\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  10, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  10 | 100/442 batches | lr 0.0000 | ms/batch 306.51 | loss 269.42 | mse 128.76 | mre 3328264.73 |gepc 131.30 |\n",
      "scGPT - INFO - | epoch  10 | 200/442 batches | lr 0.0000 | ms/batch 312.62 | loss 205.11 | mse 96.49 | mre 2744941.82 |gepc 98.77 |\n",
      "scGPT - INFO - | epoch  10 | 300/442 batches | lr 0.0000 | ms/batch 301.91 | loss 163.29 | mse 75.12 | mre 2135593.96 |gepc 76.65 |\n",
      "scGPT - INFO - | epoch  10 | 400/442 batches | lr 0.0000 | ms/batch 305.24 | loss 172.80 | mse 79.12 | mre 2242717.26 |gepc 81.87 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 140.36s | valid loss/mse 93.6191 | mre 2622951.4256\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.6191\n",
      "random masking at epoch  11, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  11 | 100/442 batches | lr 0.0000 | ms/batch 315.49 | loss 267.55 | mse 128.01 | mre 3339074.98 |gepc 130.17 |\n",
      "scGPT - INFO - | epoch  11 | 200/442 batches | lr 0.0000 | ms/batch 307.91 | loss 204.95 | mse 96.09 | mre 2686981.52 |gepc 99.04 |\n",
      "scGPT - INFO - | epoch  11 | 300/442 batches | lr 0.0000 | ms/batch 303.26 | loss 162.86 | mse 74.91 | mre 2126242.28 |gepc 76.39 |\n",
      "scGPT - INFO - | epoch  11 | 400/442 batches | lr 0.0000 | ms/batch 302.34 | loss 178.20 | mse 78.88 | mre 2265673.89 |gepc 87.61 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 140.50s | valid loss/mse 93.0592 | mre 2331153.4633\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.0592\n",
      "random masking at epoch  12, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  12 | 100/442 batches | lr 0.0000 | ms/batch 323.94 | loss 267.81 | mse 128.17 | mre 3317408.10 |gepc 130.27 |\n",
      "scGPT - INFO - | epoch  12 | 200/442 batches | lr 0.0000 | ms/batch 313.19 | loss 204.54 | mse 96.09 | mre 2663338.48 |gepc 98.74 |\n",
      "scGPT - INFO - | epoch  12 | 300/442 batches | lr 0.0000 | ms/batch 303.61 | loss 163.08 | mse 74.70 | mre 2163647.87 |gepc 77.11 |\n",
      "scGPT - INFO - | epoch  12 | 400/442 batches | lr 0.0000 | ms/batch 305.05 | loss 171.46 | mse 78.43 | mre 2167425.05 |gepc 81.94 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 142.25s | valid loss/mse 93.2694 | mre 2585169.0702\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  13, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  13 | 100/442 batches | lr 0.0000 | ms/batch 312.63 | loss 266.17 | mse 127.54 | mre 3300668.68 |gepc 129.68 |\n",
      "scGPT - INFO - | epoch  13 | 200/442 batches | lr 0.0000 | ms/batch 309.83 | loss 202.66 | mse 95.88 | mre 2683765.08 |gepc 97.53 |\n",
      "scGPT - INFO - | epoch  13 | 300/442 batches | lr 0.0000 | ms/batch 304.15 | loss 159.69 | mse 74.36 | mre 2162070.23 |gepc 75.56 |\n",
      "scGPT - INFO - | epoch  13 | 400/442 batches | lr 0.0000 | ms/batch 303.32 | loss 173.40 | mse 78.63 | mre 2207777.03 |gepc 84.61 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 140.83s | valid loss/mse 93.1553 | mre 2486218.7922\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  14, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  14 | 100/442 batches | lr 0.0000 | ms/batch 307.22 | loss 265.60 | mse 127.20 | mre 3311092.47 |gepc 129.52 |\n",
      "scGPT - INFO - | epoch  14 | 200/442 batches | lr 0.0000 | ms/batch 312.06 | loss 202.14 | mse 95.72 | mre 2684434.43 |gepc 97.74 |\n",
      "scGPT - INFO - | epoch  14 | 300/442 batches | lr 0.0000 | ms/batch 305.56 | loss 158.97 | mse 74.17 | mre 2152933.27 |gepc 75.55 |\n",
      "scGPT - INFO - | epoch  14 | 400/442 batches | lr 0.0000 | ms/batch 305.97 | loss 168.75 | mse 78.32 | mre 2216330.20 |gepc 80.55 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 140.65s | valid loss/mse 93.5000 | mre 2632847.7419\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  15, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  15 | 100/442 batches | lr 0.0000 | ms/batch 307.63 | loss 265.96 | mse 127.35 | mre 3320725.79 |gepc 129.80 |\n",
      "scGPT - INFO - | epoch  15 | 200/442 batches | lr 0.0000 | ms/batch 311.74 | loss 201.33 | mse 95.66 | mre 2689993.32 |gepc 97.12 |\n",
      "scGPT - INFO - | epoch  15 | 300/442 batches | lr 0.0000 | ms/batch 301.59 | loss 158.31 | mse 74.06 | mre 2115124.78 |gepc 74.90 |\n",
      "scGPT - INFO - | epoch  15 | 400/442 batches | lr 0.0000 | ms/batch 303.96 | loss 170.14 | mse 78.19 | mre 2215526.10 |gepc 82.15 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 140.23s | valid loss/mse 92.3779 | mre 2402311.1074\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 92.3779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482/482 [00:41<00:00, 11.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train/dab</td><td>▄▁▂▇▅▇▃█▅▆▆▄▇▄▆▇▄▃▅▂▅▅▄▅▄▄▃▃▄▄▄▃▆▃▆▂▅▄▄▅</td></tr><tr><td>train/ecs</td><td>███▇█▅▄█▄▇▆▃▅▃▆▆▂▆▃▃▄▃▄▃▂▆▃▁▄▂▂▃▁▂▂▁▂▁▂▂</td></tr><tr><td>train/mse</td><td>█▂▂▄▂▃▃▂▃▁▂▃▁▃▂▂▃▁▂▃▂▂▂▂▃▁▂▃▂▃▁▂▃▂▂▃▁▃▁▁</td></tr><tr><td>train/mvc</td><td>█▃▂▆▂▆▄▄▅▂▄▅▂▅▂▂▅▁▂▄▂▂▂▂▅▂▂▄▂▄▂▂▄▂▂▄▁▄▁▂</td></tr><tr><td>train/mvc_nzlp</td><td>▇▃▃▅▂▄▄▂▄▂▄▄▂▆▂▂▃▁▂▃▄▂▁▂▃▁▂▃▁▄▁▂█▁▃▃▂▂▁▁</td></tr><tr><td>train/nzlp</td><td>█▇▆▇▄▅▅▂▅▁▃▄▂▄▂▂▄▁▃▄▂▃▂▂▄▂▂▄▂▄▂▂▄▂▂▄▂▄▁▂</td></tr><tr><td>valid/dab</td><td>█▆▄▆▃▃▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid/mre</td><td>█▅▁▃▃▄▂▂▅▄▂▃▃▄▂</td></tr><tr><td>valid/mse</td><td>█▅▄▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid/sum_mse_dab</td><td>█▅▄▃▃▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train/dab</td><td>2.52846</td></tr><tr><td>train/ecs</td><td>6.04161</td></tr><tr><td>train/mse</td><td>76.31644</td></tr><tr><td>train/mvc</td><td>78.44629</td></tr><tr><td>train/mvc_nzlp</td><td>0.43864</td></tr><tr><td>train/nzlp</td><td>0.39117</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">graceful-sun-47</strong>: <a href=\"https://wandb.ai/madlab_sdu/scGPT/runs/2ajgbs6k\" target=\"_blank\">https://wandb.ai/madlab_sdu/scGPT/runs/2ajgbs6k</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230829_120037-2ajgbs6k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/NMF_project/reproducibility/wandb/run-20230829_123719-2hj7bv6t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/madlab_sdu/scGPT/runs/2hj7bv6t\" target=\"_blank\">cerulean-galaxy-48</a></strong> to <a href=\"https://wandb.ai/madlab_sdu/scGPT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'Mixture', 'do_train': True, 'load_model': '/work/NMF_project/reproducibility/data/scGPT/Model/', 'GEPC': True, 'ecs_thres': 0.8, 'dab_weight': 1.0, 'mask_ratio': 0.4, 'epochs': 15, 'n_bins': 51, 'lr': 0.0001, 'batch_size': 64, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True}\n",
      "save to /work/NMF_project/reproducibility/data/scGPT/FT/dev_Mixture_1\n",
      "scGPT - INFO - match 13123/13760 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /work/NMF_project/reproducibility/data/scGPT/Model/best_model.pt, the model args will be overriden by the config /work/NMF_project/reproducibility/data/scGPT/Model/args.json.\n",
      "scGPT - INFO - train set number of samples: 27747, \n",
      "\t feature length: 1201\n",
      "scGPT - INFO - valid set number of samples: 3083, \n",
      "\t feature length: 1201\n",
      "Use domain specific batchnorm with affine=False\n",
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.bias with shape torch.Size([512])\n",
      "random masking at epoch   1, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   1 | 100/442 batches | lr 0.0001 | ms/batch 300.15 | loss 499.30 | mse 248.53 | mre 5206418.83 |gepc 236.31 |\n",
      "scGPT - INFO - | epoch   1 | 200/442 batches | lr 0.0001 | ms/batch 310.84 | loss 271.79 | mse 120.15 | mre 3360254.19 |gepc 137.64 |\n",
      "scGPT - INFO - | epoch   1 | 300/442 batches | lr 0.0001 | ms/batch 295.40 | loss 197.34 | mse 86.08 | mre 2577759.15 |gepc 96.40 |\n",
      "scGPT - INFO - | epoch   1 | 400/442 batches | lr 0.0001 | ms/batch 299.05 | loss 204.07 | mse 89.74 | mre 2560098.55 |gepc 99.04 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 138.32s | valid loss/mse 124.0496 | mre 3711006.0379\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 124.0496\n",
      "random masking at epoch   2, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   2 | 100/442 batches | lr 0.0001 | ms/batch 317.15 | loss 335.84 | mse 143.73 | mre 3735079.14 |gepc 176.93 |\n",
      "scGPT - INFO - | epoch   2 | 200/442 batches | lr 0.0001 | ms/batch 309.08 | loss 227.36 | mse 103.20 | mre 2881333.90 |gepc 111.35 |\n",
      "scGPT - INFO - | epoch   2 | 300/442 batches | lr 0.0001 | ms/batch 297.88 | loss 204.98 | mse 80.88 | mre 2530047.42 |gepc 109.53 |\n",
      "scGPT - INFO - | epoch   2 | 400/442 batches | lr 0.0001 | ms/batch 301.18 | loss 208.86 | mse 85.42 | mre 2458514.40 |gepc 108.66 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 140.59s | valid loss/mse 107.0156 | mre 2596135.3680\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 107.0156\n",
      "random masking at epoch   3, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   3 | 100/442 batches | lr 0.0001 | ms/batch 303.79 | loss 299.49 | mse 135.77 | mre 3454796.64 |gepc 150.77 |\n",
      "scGPT - INFO - | epoch   3 | 200/442 batches | lr 0.0001 | ms/batch 320.34 | loss 218.31 | mse 100.38 | mre 2805825.49 |gepc 105.71 |\n",
      "scGPT - INFO - | epoch   3 | 300/442 batches | lr 0.0001 | ms/batch 300.15 | loss 182.25 | mse 79.38 | mre 2328235.18 |gepc 88.87 |\n",
      "scGPT - INFO - | epoch   3 | 400/442 batches | lr 0.0001 | ms/batch 303.68 | loss 190.22 | mse 83.01 | mre 2287177.78 |gepc 93.28 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 141.04s | valid loss/mse 101.0126 | mre 2201983.1495\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 101.0126\n",
      "random masking at epoch   4, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   4 | 100/442 batches | lr 0.0001 | ms/batch 305.89 | loss 290.07 | mse 133.05 | mre 3397380.01 |gepc 144.78 |\n",
      "scGPT - INFO - | epoch   4 | 200/442 batches | lr 0.0001 | ms/batch 310.72 | loss 214.24 | mse 98.63 | mre 2763237.79 |gepc 104.43 |\n",
      "scGPT - INFO - | epoch   4 | 300/442 batches | lr 0.0001 | ms/batch 308.08 | loss 180.34 | mse 78.72 | mre 2342462.25 |gepc 89.14 |\n",
      "scGPT - INFO - | epoch   4 | 400/442 batches | lr 0.0001 | ms/batch 306.20 | loss 210.43 | mse 81.04 | mre 2323882.27 |gepc 116.96 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 140.89s | valid loss/mse 98.6611 | mre 2659864.6667\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 98.6611\n",
      "random masking at epoch   5, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   5 | 100/442 batches | lr 0.0001 | ms/batch 308.74 | loss 306.18 | mse 132.89 | mre 3438977.02 |gepc 161.62 |\n",
      "scGPT - INFO - | epoch   5 | 200/442 batches | lr 0.0001 | ms/batch 310.13 | loss 210.62 | mse 97.71 | mre 2751993.82 |gepc 102.42 |\n",
      "scGPT - INFO - | epoch   5 | 300/442 batches | lr 0.0001 | ms/batch 308.06 | loss 194.99 | mse 77.01 | mre 2252217.07 |gepc 106.07 |\n",
      "scGPT - INFO - | epoch   5 | 400/442 batches | lr 0.0001 | ms/batch 309.80 | loss 183.82 | mse 80.95 | mre 2301969.25 |gepc 90.52 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 142.07s | valid loss/mse 98.4248 | mre 2786298.3844\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 98.4248\n",
      "random masking at epoch   6, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   6 | 100/442 batches | lr 0.0001 | ms/batch 311.36 | loss 285.17 | mse 131.95 | mre 3345228.07 |gepc 142.72 |\n",
      "scGPT - INFO - | epoch   6 | 200/442 batches | lr 0.0001 | ms/batch 311.04 | loss 211.31 | mse 97.91 | mre 2751205.27 |gepc 103.06 |\n",
      "scGPT - INFO - | epoch   6 | 300/442 batches | lr 0.0001 | ms/batch 303.41 | loss 173.12 | mse 76.57 | mre 2204562.21 |gepc 84.59 |\n",
      "scGPT - INFO - | epoch   6 | 400/442 batches | lr 0.0001 | ms/batch 304.58 | loss 194.24 | mse 80.37 | mre 2288333.28 |gepc 101.41 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 141.30s | valid loss/mse 97.2620 | mre 2710745.9493\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 97.2620\n",
      "random masking at epoch   7, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   7 | 100/442 batches | lr 0.0001 | ms/batch 306.32 | loss 276.49 | mse 130.68 | mre 3392589.04 |gepc 135.56 |\n",
      "scGPT - INFO - | epoch   7 | 200/442 batches | lr 0.0001 | ms/batch 310.23 | loss 207.40 | mse 97.31 | mre 2713642.68 |gepc 99.95 |\n",
      "scGPT - INFO - | epoch   7 | 300/442 batches | lr 0.0001 | ms/batch 307.34 | loss 164.90 | mse 75.76 | mre 2162416.99 |gepc 77.40 |\n",
      "scGPT - INFO - | epoch   7 | 400/442 batches | lr 0.0001 | ms/batch 306.08 | loss 183.51 | mse 80.90 | mre 2247962.88 |gepc 90.26 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 141.35s | valid loss/mse 96.3322 | mre 2233406.6778\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 96.3322\n",
      "random masking at epoch   8, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   8 | 100/442 batches | lr 0.0000 | ms/batch 306.15 | loss 273.04 | mse 129.61 | mre 3324958.36 |gepc 133.54 |\n",
      "scGPT - INFO - | epoch   8 | 200/442 batches | lr 0.0000 | ms/batch 308.09 | loss 206.45 | mse 97.27 | mre 2749351.33 |gepc 99.05 |\n",
      "scGPT - INFO - | epoch   8 | 300/442 batches | lr 0.0000 | ms/batch 301.40 | loss 167.02 | mse 75.63 | mre 2180130.02 |gepc 79.61 |\n",
      "scGPT - INFO - | epoch   8 | 400/442 batches | lr 0.0000 | ms/batch 302.23 | loss 178.35 | mse 79.22 | mre 2229486.79 |gepc 87.06 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 139.52s | valid loss/mse 95.6107 | mre 2515482.4309\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 95.6107\n",
      "random masking at epoch   9, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   9 | 100/442 batches | lr 0.0000 | ms/batch 305.10 | loss 274.39 | mse 129.40 | mre 3401635.25 |gepc 135.37 |\n",
      "scGPT - INFO - | epoch   9 | 200/442 batches | lr 0.0000 | ms/batch 306.95 | loss 205.05 | mse 96.67 | mre 2678715.78 |gepc 98.61 |\n",
      "scGPT - INFO - | epoch   9 | 300/442 batches | lr 0.0000 | ms/batch 303.35 | loss 162.35 | mse 74.78 | mre 2151950.47 |gepc 75.92 |\n",
      "scGPT - INFO - | epoch   9 | 400/442 batches | lr 0.0000 | ms/batch 305.61 | loss 181.48 | mse 79.09 | mre 2187217.67 |gepc 90.44 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 140.42s | valid loss/mse 96.0456 | mre 2814965.9454\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  10, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  10 | 100/442 batches | lr 0.0000 | ms/batch 325.28 | loss 269.70 | mse 128.59 | mre 3389630.75 |gepc 131.66 |\n",
      "scGPT - INFO - | epoch  10 | 200/442 batches | lr 0.0000 | ms/batch 312.83 | loss 204.88 | mse 96.35 | mre 2704165.01 |gepc 98.72 |\n",
      "scGPT - INFO - | epoch  10 | 300/442 batches | lr 0.0000 | ms/batch 306.30 | loss 162.20 | mse 74.77 | mre 2163520.47 |gepc 75.89 |\n",
      "scGPT - INFO - | epoch  10 | 400/442 batches | lr 0.0000 | ms/batch 303.97 | loss 177.18 | mse 78.81 | mre 2188828.97 |gepc 86.36 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 142.62s | valid loss/mse 94.8235 | mre 2756653.0019\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 94.8235\n",
      "random masking at epoch  11, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  11 | 100/442 batches | lr 0.0000 | ms/batch 308.17 | loss 267.92 | mse 127.97 | mre 3345137.93 |gepc 130.62 |\n",
      "scGPT - INFO - | epoch  11 | 200/442 batches | lr 0.0000 | ms/batch 311.25 | loss 203.80 | mse 95.99 | mre 2704195.69 |gepc 97.98 |\n",
      "scGPT - INFO - | epoch  11 | 300/442 batches | lr 0.0000 | ms/batch 303.08 | loss 162.82 | mse 74.62 | mre 2138448.78 |gepc 76.81 |\n",
      "scGPT - INFO - | epoch  11 | 400/442 batches | lr 0.0000 | ms/batch 305.25 | loss 179.53 | mse 78.68 | mre 2170870.63 |gepc 88.69 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 140.74s | valid loss/mse 93.6669 | mre 2318136.2457\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.6669\n",
      "random masking at epoch  12, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  12 | 100/442 batches | lr 0.0000 | ms/batch 312.12 | loss 267.75 | mse 128.11 | mre 3324653.92 |gepc 130.38 |\n",
      "scGPT - INFO - | epoch  12 | 200/442 batches | lr 0.0000 | ms/batch 312.21 | loss 202.86 | mse 95.82 | mre 2685748.69 |gepc 97.36 |\n",
      "scGPT - INFO - | epoch  12 | 300/442 batches | lr 0.0000 | ms/batch 302.96 | loss 162.54 | mse 74.98 | mre 2144983.83 |gepc 76.29 |\n",
      "scGPT - INFO - | epoch  12 | 400/442 batches | lr 0.0000 | ms/batch 307.34 | loss 175.83 | mse 78.76 | mre 2172552.52 |gepc 84.98 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 141.38s | valid loss/mse 93.5321 | mre 2598246.1629\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.5321\n",
      "random masking at epoch  13, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  13 | 100/442 batches | lr 0.0000 | ms/batch 307.66 | loss 266.47 | mse 127.61 | mre 3345358.24 |gepc 129.76 |\n",
      "scGPT - INFO - | epoch  13 | 200/442 batches | lr 0.0000 | ms/batch 309.42 | loss 203.78 | mse 95.93 | mre 2659066.17 |gepc 98.31 |\n",
      "scGPT - INFO - | epoch  13 | 300/442 batches | lr 0.0000 | ms/batch 301.30 | loss 160.39 | mse 74.21 | mre 2140886.34 |gepc 74.97 |\n",
      "scGPT - INFO - | epoch  13 | 400/442 batches | lr 0.0000 | ms/batch 304.05 | loss 171.38 | mse 78.30 | mre 2217662.59 |gepc 81.75 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 140.17s | valid loss/mse 93.0727 | mre 2484291.9749\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.0727\n",
      "random masking at epoch  14, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  14 | 100/442 batches | lr 0.0000 | ms/batch 311.54 | loss 267.21 | mse 127.34 | mre 3342266.27 |gepc 131.07 |\n",
      "scGPT - INFO - | epoch  14 | 200/442 batches | lr 0.0000 | ms/batch 312.25 | loss 201.82 | mse 95.62 | mre 2677141.72 |gepc 97.23 |\n",
      "scGPT - INFO - | epoch  14 | 300/442 batches | lr 0.0000 | ms/batch 309.41 | loss 160.43 | mse 74.47 | mre 2112350.31 |gepc 76.18 |\n",
      "scGPT - INFO - | epoch  14 | 400/442 batches | lr 0.0000 | ms/batch 305.88 | loss 170.25 | mse 78.23 | mre 2233500.45 |gepc 81.98 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 141.85s | valid loss/mse 93.3020 | mre 2628717.6594\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  15, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  15 | 100/442 batches | lr 0.0000 | ms/batch 319.26 | loss 266.26 | mse 127.28 | mre 3310515.81 |gepc 130.24 |\n",
      "scGPT - INFO - | epoch  15 | 200/442 batches | lr 0.0000 | ms/batch 311.99 | loss 201.36 | mse 95.60 | mre 2697096.76 |gepc 97.18 |\n",
      "scGPT - INFO - | epoch  15 | 300/442 batches | lr 0.0000 | ms/batch 302.15 | loss 157.94 | mse 73.85 | mre 2109836.58 |gepc 74.77 |\n",
      "scGPT - INFO - | epoch  15 | 400/442 batches | lr 0.0000 | ms/batch 306.06 | loss 168.01 | mse 78.02 | mre 2200357.56 |gepc 80.22 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 141.93s | valid loss/mse 92.6078 | mre 2491156.4526\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 92.6078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482/482 [00:41<00:00, 11.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train/dab</td><td>▄▁▃▇▃▇▁▅▆▇▆▄█▆▅▆▆▃▅▂▄▄▄▅▄▄▃▃▅▄▄▅▆▃▆▂▄▃▅▅</td></tr><tr><td>train/ecs</td><td>█████▄▄█▄▆▅▄▅▃▆▆▂▆▃▃▄▃▄▃▂▆▃▁▄▂▃▃▁▄▄▁▃▁▂▃</td></tr><tr><td>train/mse</td><td>█▂▂▄▂▃▃▂▃▁▂▃▁▄▂▂▃▁▂▃▂▂▂▂▃▁▂▃▂▃▁▂▃▂▂▃▁▃▁▁</td></tr><tr><td>train/mvc</td><td>▄▂▂▃▁▃▃▁▃▁▁▄█▄▁▁▃▁▂▃▁▂▁▁▃▁▁▂▁▃▁▁▃▁▁▂▁▃▁▁</td></tr><tr><td>train/mvc_nzlp</td><td>█▃▃█▂▇▄▂▄▂▃▆▂▇▃▅▄▁▂▃▂▂▂▂▄▂▂▃▁▄▁▂▃▁▂▂▁▃▁▁</td></tr><tr><td>train/nzlp</td><td>█▇▆▇▄▆▅▂▅▂▂▄▂▅▂▂▅▁▃▄▂▃▂▂▄▂▂▄▂▄▂▂▄▂▂▄▂▄▁▂</td></tr><tr><td>valid/dab</td><td>█▅▅▃▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid/mre</td><td>█▃▁▃▄▃▁▂▄▄▂▃▂▃▂</td></tr><tr><td>valid/mse</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid/sum_mse_dab</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train/dab</td><td>2.15723</td></tr><tr><td>train/ecs</td><td>6.56093</td></tr><tr><td>train/mse</td><td>75.89831</td></tr><tr><td>train/mvc</td><td>78.14529</td></tr><tr><td>train/mvc_nzlp</td><td>0.43278</td></tr><tr><td>train/nzlp</td><td>0.38992</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cerulean-galaxy-48</strong>: <a href=\"https://wandb.ai/madlab_sdu/scGPT/runs/2hj7bv6t\" target=\"_blank\">https://wandb.ai/madlab_sdu/scGPT/runs/2hj7bv6t</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230829_123719-2hj7bv6t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/NMF_project/reproducibility/wandb/run-20230829_131406-3ucdyb1q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/madlab_sdu/scGPT/runs/3ucdyb1q\" target=\"_blank\">hopeful-gorge-49</a></strong> to <a href=\"https://wandb.ai/madlab_sdu/scGPT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'Mixture', 'do_train': True, 'load_model': '/work/NMF_project/reproducibility/data/scGPT/Model/', 'GEPC': True, 'ecs_thres': 0.8, 'dab_weight': 1.0, 'mask_ratio': 0.4, 'epochs': 15, 'n_bins': 51, 'lr': 0.0001, 'batch_size': 64, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True}\n",
      "save to /work/NMF_project/reproducibility/data/scGPT/FT/dev_Mixture_2\n",
      "scGPT - INFO - match 13123/13760 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /work/NMF_project/reproducibility/data/scGPT/Model/best_model.pt, the model args will be overriden by the config /work/NMF_project/reproducibility/data/scGPT/Model/args.json.\n",
      "scGPT - INFO - train set number of samples: 27747, \n",
      "\t feature length: 1201\n",
      "scGPT - INFO - valid set number of samples: 3083, \n",
      "\t feature length: 1201\n",
      "Use domain specific batchnorm with affine=False\n",
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.bias with shape torch.Size([512])\n",
      "random masking at epoch   1, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   1 | 100/442 batches | lr 0.0001 | ms/batch 301.29 | loss 499.30 | mse 248.54 | mre 5208390.82 |gepc 236.31 |\n",
      "scGPT - INFO - | epoch   1 | 200/442 batches | lr 0.0001 | ms/batch 302.49 | loss 266.58 | mse 117.83 | mre 3278751.78 |gepc 134.66 |\n",
      "scGPT - INFO - | epoch   1 | 300/442 batches | lr 0.0001 | ms/batch 297.75 | loss 193.34 | mse 85.21 | mre 2551738.50 |gepc 93.44 |\n",
      "scGPT - INFO - | epoch   1 | 400/442 batches | lr 0.0001 | ms/batch 301.60 | loss 200.82 | mse 87.44 | mre 2476375.26 |gepc 98.42 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 137.59s | valid loss/mse 122.9594 | mre 3486221.8971\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 122.9594\n",
      "random masking at epoch   2, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   2 | 100/442 batches | lr 0.0001 | ms/batch 302.83 | loss 325.50 | mse 142.61 | mre 3702930.15 |gepc 167.95 |\n",
      "scGPT - INFO - | epoch   2 | 200/442 batches | lr 0.0001 | ms/batch 307.78 | loss 235.74 | mse 104.79 | mre 2860544.21 |gepc 117.53 |\n",
      "scGPT - INFO - | epoch   2 | 300/442 batches | lr 0.0001 | ms/batch 298.38 | loss 179.27 | mse 80.06 | mre 2334461.70 |gepc 85.49 |\n",
      "scGPT - INFO - | epoch   2 | 400/442 batches | lr 0.0001 | ms/batch 303.40 | loss 195.20 | mse 84.62 | mre 2443596.72 |gepc 96.21 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 138.67s | valid loss/mse 107.2448 | mre 2308008.5147\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 107.2448\n",
      "random masking at epoch   3, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   3 | 100/442 batches | lr 0.0001 | ms/batch 302.34 | loss 300.37 | mse 136.21 | mre 3449654.51 |gepc 151.56 |\n",
      "scGPT - INFO - | epoch   3 | 200/442 batches | lr 0.0001 | ms/batch 306.69 | loss 221.74 | mse 100.48 | mre 2775777.83 |gepc 108.72 |\n",
      "scGPT - INFO - | epoch   3 | 300/442 batches | lr 0.0001 | ms/batch 320.27 | loss 181.34 | mse 79.06 | mre 2370968.26 |gepc 88.39 |\n",
      "scGPT - INFO - | epoch   3 | 400/442 batches | lr 0.0001 | ms/batch 300.23 | loss 191.50 | mse 84.73 | mre 2443563.65 |gepc 92.75 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 141.10s | valid loss/mse 101.2573 | mre 2382509.5553\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 101.2573\n",
      "random masking at epoch   4, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   4 | 100/442 batches | lr 0.0001 | ms/batch 337.82 | loss 296.20 | mse 133.85 | mre 3450156.45 |gepc 149.64 |\n",
      "scGPT - INFO - | epoch   4 | 200/442 batches | lr 0.0001 | ms/batch 336.14 | loss 214.62 | mse 98.94 | mre 2792311.66 |gepc 103.93 |\n",
      "scGPT - INFO - | epoch   4 | 300/442 batches | lr 0.0001 | ms/batch 308.49 | loss 172.94 | mse 77.59 | mre 2259335.49 |gepc 82.28 |\n",
      "scGPT - INFO - | epoch   4 | 400/442 batches | lr 0.0001 | ms/batch 314.73 | loss 182.55 | mse 81.69 | mre 2300942.94 |gepc 88.17 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 147.83s | valid loss/mse 101.7784 | mre 2890502.2125\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   5, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   5 | 100/442 batches | lr 0.0001 | ms/batch 325.34 | loss 283.61 | mse 131.33 | mre 3428060.80 |gepc 140.35 |\n",
      "scGPT - INFO - | epoch   5 | 200/442 batches | lr 0.0001 | ms/batch 312.60 | loss 209.83 | mse 97.72 | mre 2734606.97 |gepc 101.45 |\n",
      "scGPT - INFO - | epoch   5 | 300/442 batches | lr 0.0001 | ms/batch 304.30 | loss 168.51 | mse 77.68 | mre 2187135.45 |gepc 79.00 |\n",
      "scGPT - INFO - | epoch   5 | 400/442 batches | lr 0.0001 | ms/batch 304.64 | loss 180.26 | mse 82.03 | mre 2356403.42 |gepc 85.71 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 142.99s | valid loss/mse 98.3201 | mre 2583313.3821\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 98.3201\n",
      "random masking at epoch   6, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   6 | 100/442 batches | lr 0.0001 | ms/batch 311.73 | loss 281.25 | mse 130.78 | mre 3447820.67 |gepc 139.72 |\n",
      "scGPT - INFO - | epoch   6 | 200/442 batches | lr 0.0001 | ms/batch 306.20 | loss 208.31 | mse 97.80 | mre 2738832.09 |gepc 100.25 |\n",
      "scGPT - INFO - | epoch   6 | 300/442 batches | lr 0.0001 | ms/batch 304.99 | loss 170.97 | mse 76.67 | mre 2201347.32 |gepc 82.34 |\n",
      "scGPT - INFO - | epoch   6 | 400/442 batches | lr 0.0001 | ms/batch 306.30 | loss 185.47 | mse 80.54 | mre 2225163.77 |gepc 92.28 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 140.93s | valid loss/mse 97.3215 | mre 2730502.4614\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 97.3215\n",
      "random masking at epoch   7, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   7 | 100/442 batches | lr 0.0001 | ms/batch 310.97 | loss 275.00 | mse 130.41 | mre 3362878.31 |gepc 134.17 |\n",
      "scGPT - INFO - | epoch   7 | 200/442 batches | lr 0.0001 | ms/batch 317.24 | loss 208.33 | mse 97.38 | mre 2735857.59 |gepc 100.77 |\n",
      "scGPT - INFO - | epoch   7 | 300/442 batches | lr 0.0001 | ms/batch 305.29 | loss 167.20 | mse 75.74 | mre 2183849.58 |gepc 79.64 |\n",
      "scGPT - INFO - | epoch   7 | 400/442 batches | lr 0.0001 | ms/batch 308.68 | loss 175.51 | mse 79.63 | mre 2246437.61 |gepc 83.69 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 142.32s | valid loss/mse 97.4776 | mre 2893031.6909\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   8, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   8 | 100/442 batches | lr 0.0000 | ms/batch 304.12 | loss 274.06 | mse 129.72 | mre 3385477.58 |gepc 134.50 |\n",
      "scGPT - INFO - | epoch   8 | 200/442 batches | lr 0.0000 | ms/batch 309.00 | loss 208.06 | mse 97.06 | mre 2756645.44 |gepc 100.93 |\n",
      "scGPT - INFO - | epoch   8 | 300/442 batches | lr 0.0000 | ms/batch 304.44 | loss 174.61 | mse 75.46 | mre 2165162.26 |gepc 87.48 |\n",
      "scGPT - INFO - | epoch   8 | 400/442 batches | lr 0.0000 | ms/batch 304.93 | loss 175.22 | mse 79.11 | mre 2247597.40 |gepc 84.17 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 139.70s | valid loss/mse 96.8753 | mre 2852337.6806\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 96.8753\n",
      "random masking at epoch   9, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   9 | 100/442 batches | lr 0.0000 | ms/batch 309.44 | loss 274.27 | mse 129.34 | mre 3372529.99 |gepc 135.01 |\n",
      "scGPT - INFO - | epoch   9 | 200/442 batches | lr 0.0000 | ms/batch 304.14 | loss 210.96 | mse 96.90 | mre 2691460.54 |gepc 103.94 |\n",
      "scGPT - INFO - | epoch   9 | 300/442 batches | lr 0.0000 | ms/batch 298.80 | loss 164.42 | mse 75.03 | mre 2157584.00 |gepc 77.63 |\n",
      "scGPT - INFO - | epoch   9 | 400/442 batches | lr 0.0000 | ms/batch 306.80 | loss 178.58 | mse 79.39 | mre 2262197.82 |gepc 87.44 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 139.73s | valid loss/mse 96.0009 | mre 2834998.1028\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 96.0009\n",
      "random masking at epoch  10, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  10 | 100/442 batches | lr 0.0000 | ms/batch 312.85 | loss 270.15 | mse 128.56 | mre 3346441.55 |gepc 132.22 |\n",
      "scGPT - INFO - | epoch  10 | 200/442 batches | lr 0.0000 | ms/batch 321.94 | loss 205.68 | mse 96.36 | mre 2710116.37 |gepc 99.21 |\n",
      "scGPT - INFO - | epoch  10 | 300/442 batches | lr 0.0000 | ms/batch 303.17 | loss 163.63 | mse 75.04 | mre 2146498.07 |gepc 76.96 |\n",
      "scGPT - INFO - | epoch  10 | 400/442 batches | lr 0.0000 | ms/batch 306.52 | loss 177.05 | mse 78.89 | mre 2234899.35 |gepc 86.45 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 142.54s | valid loss/mse 95.3972 | mre 2678657.9148\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 95.3972\n",
      "random masking at epoch  11, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  11 | 100/442 batches | lr 0.0000 | ms/batch 313.28 | loss 270.32 | mse 129.61 | mre 3378934.04 |gepc 131.37 |\n",
      "scGPT - INFO - | epoch  11 | 200/442 batches | lr 0.0000 | ms/batch 311.54 | loss 203.39 | mse 96.04 | mre 2674239.40 |gepc 97.59 |\n",
      "scGPT - INFO - | epoch  11 | 300/442 batches | lr 0.0000 | ms/batch 303.44 | loss 161.57 | mse 74.47 | mre 2144833.09 |gepc 75.60 |\n",
      "scGPT - INFO - | epoch  11 | 400/442 batches | lr 0.0000 | ms/batch 304.66 | loss 175.48 | mse 78.68 | mre 2233647.80 |gepc 85.13 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 141.50s | valid loss/mse 93.8796 | mre 2408484.2242\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.8796\n",
      "random masking at epoch  12, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  12 | 100/442 batches | lr 0.0000 | ms/batch 308.30 | loss 268.60 | mse 128.59 | mre 3322728.97 |gepc 130.74 |\n",
      "scGPT - INFO - | epoch  12 | 200/442 batches | lr 0.0000 | ms/batch 311.73 | loss 202.93 | mse 95.93 | mre 2663707.99 |gepc 97.60 |\n",
      "scGPT - INFO - | epoch  12 | 300/442 batches | lr 0.0000 | ms/batch 307.56 | loss 160.83 | mse 74.77 | mre 2156143.77 |gepc 76.06 |\n",
      "scGPT - INFO - | epoch  12 | 400/442 batches | lr 0.0000 | ms/batch 309.50 | loss 169.68 | mse 78.46 | mre 2207835.75 |gepc 81.02 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 142.11s | valid loss/mse 93.1246 | mre 2395700.3857\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.1246\n",
      "random masking at epoch  13, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  13 | 100/442 batches | lr 0.0000 | ms/batch 305.35 | loss 267.94 | mse 127.91 | mre 3313169.15 |gepc 130.97 |\n",
      "scGPT - INFO - | epoch  13 | 200/442 batches | lr 0.0000 | ms/batch 313.62 | loss 202.70 | mse 95.89 | mre 2688188.14 |gepc 97.97 |\n",
      "scGPT - INFO - | epoch  13 | 300/442 batches | lr 0.0000 | ms/batch 307.13 | loss 159.79 | mse 74.38 | mre 2120884.42 |gepc 76.04 |\n",
      "scGPT - INFO - | epoch  13 | 400/442 batches | lr 0.0000 | ms/batch 306.56 | loss 169.01 | mse 78.28 | mre 2210161.03 |gepc 80.83 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 141.37s | valid loss/mse 92.9900 | mre 2487889.2777\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 92.9900\n",
      "random masking at epoch  14, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  14 | 100/442 batches | lr 0.0000 | ms/batch 305.70 | loss 267.31 | mse 127.53 | mre 3306792.71 |gepc 130.86 |\n",
      "scGPT - INFO - | epoch  14 | 200/442 batches | lr 0.0000 | ms/batch 308.36 | loss 201.94 | mse 95.69 | mre 2684521.45 |gepc 97.58 |\n",
      "scGPT - INFO - | epoch  14 | 300/442 batches | lr 0.0000 | ms/batch 303.55 | loss 158.99 | mse 74.06 | mre 2120524.92 |gepc 75.70 |\n",
      "scGPT - INFO - | epoch  14 | 400/442 batches | lr 0.0000 | ms/batch 302.48 | loss 169.46 | mse 78.16 | mre 2218942.26 |gepc 81.53 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 140.46s | valid loss/mse 92.6559 | mre 2478878.5497\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 92.6559\n",
      "random masking at epoch  15, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  15 | 100/442 batches | lr 0.0000 | ms/batch 306.57 | loss 266.36 | mse 127.31 | mre 3325692.25 |gepc 130.26 |\n",
      "scGPT - INFO - | epoch  15 | 200/442 batches | lr 0.0000 | ms/batch 325.36 | loss 201.18 | mse 95.60 | mre 2678283.21 |gepc 96.99 |\n",
      "scGPT - INFO - | epoch  15 | 300/442 batches | lr 0.0000 | ms/batch 310.23 | loss 158.42 | mse 73.77 | mre 2086882.14 |gepc 75.42 |\n",
      "scGPT - INFO - | epoch  15 | 400/442 batches | lr 0.0000 | ms/batch 309.96 | loss 167.52 | mse 78.03 | mre 2210096.67 |gepc 79.76 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 143.18s | valid loss/mse 92.3828 | mre 2546194.2802\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 92.3828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482/482 [00:45<00:00, 10.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train/dab</td><td>▄▁▃█▄▇▂▄▆█▄▅█▅▆█▅▃▆▂▆▅▆▅▅▅▃▃▄▄▄▄▆▄▆▂▅▃▅▆</td></tr><tr><td>train/ecs</td><td>███▇█▄▄█▄▇▆▄▅▂▆▆▃▆▄▃▄▃▄▃▂▆▃▁▄▂▂▂▁▂▂▁▃▁▂▂</td></tr><tr><td>train/mse</td><td>█▂▂▄▂▃▃▁▃▁▂▃▁▃▁▂▃▁▂▃▂▂▂▂▃▁▂▃▂▃▁▂▃▂▂▃▁▃▁▁</td></tr><tr><td>train/mvc</td><td>█▃▃▆▂▅▄▂▅▂▂▅▂▅▂▂▅▁▄▄▂▂▂▂▅▂▂▄▂▄▂▂▅▂▂▄▁▄▁▂</td></tr><tr><td>train/mvc_nzlp</td><td>▇▄▃▅▂▄▄▂▄▁▂▃▂▄▂▃█▁▂▃▂▂▂▂▃▁▂▃▂▃▂▂▃▁▂▂▁▂▁▁</td></tr><tr><td>train/nzlp</td><td>█▇▆▇▄▆▅▂▅▁▂▄▂▄▂▂▄▁▃▄▂▃▂▂▄▂▃▄▂▄▂▂▄▂▂▄▂▄▁▂</td></tr><tr><td>valid/dab</td><td>█▅▅▅▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>valid/mre</td><td>█▁▁▄▃▄▄▄▄▃▂▂▂▂▂</td></tr><tr><td>valid/mse</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>valid/sum_mse_dab</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train/dab</td><td>2.48493</td></tr><tr><td>train/ecs</td><td>5.90701</td></tr><tr><td>train/mse</td><td>76.14938</td></tr><tr><td>train/mvc</td><td>78.35318</td></tr><tr><td>train/mvc_nzlp</td><td>0.43936</td></tr><tr><td>train/nzlp</td><td>0.39121</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hopeful-gorge-49</strong>: <a href=\"https://wandb.ai/madlab_sdu/scGPT/runs/3ucdyb1q\" target=\"_blank\">https://wandb.ai/madlab_sdu/scGPT/runs/3ucdyb1q</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230829_131406-3ucdyb1q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/NMF_project/reproducibility/wandb/run-20230829_135103-134qqtnn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/madlab_sdu/scGPT/runs/134qqtnn\" target=\"_blank\">gentle-microwave-50</a></strong> to <a href=\"https://wandb.ai/madlab_sdu/scGPT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'Mixture', 'do_train': True, 'load_model': '/work/NMF_project/reproducibility/data/scGPT/Model/', 'GEPC': True, 'ecs_thres': 0.8, 'dab_weight': 1.0, 'mask_ratio': 0.4, 'epochs': 15, 'n_bins': 51, 'lr': 0.0001, 'batch_size': 64, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True}\n",
      "save to /work/NMF_project/reproducibility/data/scGPT/FT/dev_Mixture_3\n",
      "scGPT - INFO - match 13123/13760 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /work/NMF_project/reproducibility/data/scGPT/Model/best_model.pt, the model args will be overriden by the config /work/NMF_project/reproducibility/data/scGPT/Model/args.json.\n",
      "scGPT - INFO - train set number of samples: 27747, \n",
      "\t feature length: 1201\n",
      "scGPT - INFO - valid set number of samples: 3083, \n",
      "\t feature length: 1201\n",
      "Use domain specific batchnorm with affine=False\n",
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.bias with shape torch.Size([512])\n",
      "random masking at epoch   1, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   1 | 100/442 batches | lr 0.0001 | ms/batch 301.24 | loss 498.93 | mse 248.36 | mre 5203900.05 |gepc 236.13 |\n",
      "scGPT - INFO - | epoch   1 | 200/442 batches | lr 0.0001 | ms/batch 311.97 | loss 262.85 | mse 117.56 | mre 3273107.29 |gepc 131.23 |\n",
      "scGPT - INFO - | epoch   1 | 300/442 batches | lr 0.0001 | ms/batch 297.71 | loss 190.21 | mse 84.06 | mre 2476994.33 |gepc 91.80 |\n",
      "scGPT - INFO - | epoch   1 | 400/442 batches | lr 0.0001 | ms/batch 302.77 | loss 201.94 | mse 88.69 | mre 2448497.41 |gepc 98.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 138.89s | valid loss/mse 118.7536 | mre 3172679.6455\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 118.7536\n",
      "random masking at epoch   2, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   2 | 100/442 batches | lr 0.0001 | ms/batch 308.52 | loss 322.00 | mse 140.65 | mre 3677290.81 |gepc 166.71 |\n",
      "scGPT - INFO - | epoch   2 | 200/442 batches | lr 0.0001 | ms/batch 305.88 | loss 232.51 | mse 104.15 | mre 2858879.19 |gepc 115.60 |\n",
      "scGPT - INFO - | epoch   2 | 300/442 batches | lr 0.0001 | ms/batch 298.38 | loss 177.06 | mse 79.49 | mre 2331096.01 |gepc 84.16 |\n",
      "scGPT - INFO - | epoch   2 | 400/442 batches | lr 0.0001 | ms/batch 305.17 | loss 226.59 | mse 84.75 | mre 2528969.11 |gepc 126.78 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 139.70s | valid loss/mse 104.9091 | mre 2387328.7286\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 104.9091\n",
      "random masking at epoch   3, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   3 | 100/442 batches | lr 0.0001 | ms/batch 306.34 | loss 301.72 | mse 135.38 | mre 3462037.72 |gepc 153.72 |\n",
      "scGPT - INFO - | epoch   3 | 200/442 batches | lr 0.0001 | ms/batch 313.66 | loss 219.20 | mse 101.30 | mre 2813113.26 |gepc 105.56 |\n",
      "scGPT - INFO - | epoch   3 | 300/442 batches | lr 0.0001 | ms/batch 313.82 | loss 184.33 | mse 79.09 | mre 2346253.45 |gepc 91.47 |\n",
      "scGPT - INFO - | epoch   3 | 400/442 batches | lr 0.0001 | ms/batch 300.93 | loss 226.85 | mse 83.23 | mre 2484014.42 |gepc 129.10 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 141.32s | valid loss/mse 101.7083 | mre 2294350.0806\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 101.7083\n",
      "random masking at epoch   4, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   4 | 100/442 batches | lr 0.0001 | ms/batch 314.92 | loss 296.48 | mse 133.71 | mre 3434241.21 |gepc 150.50 |\n",
      "scGPT - INFO - | epoch   4 | 200/442 batches | lr 0.0001 | ms/batch 321.06 | loss 220.67 | mse 99.81 | mre 2778410.95 |gepc 108.55 |\n",
      "scGPT - INFO - | epoch   4 | 300/442 batches | lr 0.0001 | ms/batch 310.87 | loss 177.25 | mse 78.62 | mre 2299714.60 |gepc 85.85 |\n",
      "scGPT - INFO - | epoch   4 | 400/442 batches | lr 0.0001 | ms/batch 310.89 | loss 214.03 | mse 82.85 | mre 2300068.60 |gepc 117.57 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 143.54s | valid loss/mse 101.7123 | mre 3047529.4872\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   5, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   5 | 100/442 batches | lr 0.0001 | ms/batch 310.25 | loss 282.67 | mse 131.10 | mre 3380246.76 |gepc 140.18 |\n",
      "scGPT - INFO - | epoch   5 | 200/442 batches | lr 0.0001 | ms/batch 309.09 | loss 216.15 | mse 98.44 | mre 2819151.21 |gepc 106.49 |\n",
      "scGPT - INFO - | epoch   5 | 300/442 batches | lr 0.0001 | ms/batch 302.89 | loss 170.21 | mse 77.72 | mre 2286078.10 |gepc 80.84 |\n",
      "scGPT - INFO - | epoch   5 | 400/442 batches | lr 0.0001 | ms/batch 309.99 | loss 225.45 | mse 82.82 | mre 2352608.75 |gepc 129.80 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 140.81s | valid loss/mse 98.8188 | mre 2774866.7980\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 98.8188\n",
      "random masking at epoch   6, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   6 | 100/442 batches | lr 0.0001 | ms/batch 306.88 | loss 275.37 | mse 130.19 | mre 3383775.80 |gepc 134.58 |\n",
      "scGPT - INFO - | epoch   6 | 200/442 batches | lr 0.0001 | ms/batch 310.32 | loss 212.86 | mse 98.53 | mre 2696874.35 |gepc 103.50 |\n",
      "scGPT - INFO - | epoch   6 | 300/442 batches | lr 0.0001 | ms/batch 309.87 | loss 170.47 | mse 77.18 | mre 2228002.16 |gepc 81.54 |\n",
      "scGPT - INFO - | epoch   6 | 400/442 batches | lr 0.0001 | ms/batch 303.87 | loss 187.15 | mse 81.28 | mre 2222598.26 |gepc 93.09 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 140.94s | valid loss/mse 96.9438 | mre 2620849.2315\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 96.9438\n",
      "random masking at epoch   7, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   7 | 100/442 batches | lr 0.0001 | ms/batch 307.05 | loss 275.35 | mse 131.32 | mre 3325171.88 |gepc 133.88 |\n",
      "scGPT - INFO - | epoch   7 | 200/442 batches | lr 0.0001 | ms/batch 315.37 | loss 211.10 | mse 98.52 | mre 2813233.74 |gepc 101.80 |\n",
      "scGPT - INFO - | epoch   7 | 300/442 batches | lr 0.0001 | ms/batch 300.20 | loss 167.18 | mse 76.18 | mre 2196095.04 |gepc 79.31 |\n",
      "scGPT - INFO - | epoch   7 | 400/442 batches | lr 0.0001 | ms/batch 311.21 | loss 194.64 | mse 80.67 | mre 2361630.38 |gepc 101.39 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 141.33s | valid loss/mse 95.9353 | mre 2354306.4891\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 95.9353\n",
      "random masking at epoch   8, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   8 | 100/442 batches | lr 0.0000 | ms/batch 306.36 | loss 276.45 | mse 130.74 | mre 3348975.65 |gepc 135.75 |\n",
      "scGPT - INFO - | epoch   8 | 200/442 batches | lr 0.0000 | ms/batch 313.35 | loss 207.85 | mse 97.42 | mre 2752828.31 |gepc 100.39 |\n",
      "scGPT - INFO - | epoch   8 | 300/442 batches | lr 0.0000 | ms/batch 304.24 | loss 169.06 | mse 75.81 | mre 2142969.83 |gepc 81.54 |\n",
      "scGPT - INFO - | epoch   8 | 400/442 batches | lr 0.0000 | ms/batch 304.78 | loss 187.13 | mse 79.70 | mre 2248792.96 |gepc 95.00 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 140.76s | valid loss/mse 94.6963 | mre 2434435.6776\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 94.6963\n",
      "random masking at epoch   9, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   9 | 100/442 batches | lr 0.0000 | ms/batch 309.49 | loss 272.27 | mse 129.47 | mre 3314689.46 |gepc 133.15 |\n",
      "scGPT - INFO - | epoch   9 | 200/442 batches | lr 0.0000 | ms/batch 308.83 | loss 205.10 | mse 96.64 | mre 2683379.16 |gepc 98.80 |\n",
      "scGPT - INFO - | epoch   9 | 300/442 batches | lr 0.0000 | ms/batch 301.22 | loss 164.18 | mse 74.78 | mre 2170757.09 |gepc 77.99 |\n",
      "scGPT - INFO - | epoch   9 | 400/442 batches | lr 0.0000 | ms/batch 300.81 | loss 180.44 | mse 79.63 | mre 2213519.72 |gepc 88.34 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 139.84s | valid loss/mse 94.3808 | mre 2628694.8263\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 94.3808\n",
      "random masking at epoch  10, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  10 | 100/442 batches | lr 0.0000 | ms/batch 306.85 | loss 270.60 | mse 128.35 | mre 3373937.77 |gepc 132.50 |\n",
      "scGPT - INFO - | epoch  10 | 200/442 batches | lr 0.0000 | ms/batch 308.27 | loss 204.72 | mse 96.32 | mre 2697979.01 |gepc 98.73 |\n",
      "scGPT - INFO - | epoch  10 | 300/442 batches | lr 0.0000 | ms/batch 313.90 | loss 166.95 | mse 75.49 | mre 2206016.23 |gepc 79.81 |\n",
      "scGPT - INFO - | epoch  10 | 400/442 batches | lr 0.0000 | ms/batch 309.19 | loss 174.46 | mse 79.07 | mre 2205826.47 |gepc 83.17 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 141.94s | valid loss/mse 93.8323 | mre 2613661.0532\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.8323\n",
      "random masking at epoch  11, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  11 | 100/442 batches | lr 0.0000 | ms/batch 310.02 | loss 270.24 | mse 127.94 | mre 3331425.02 |gepc 133.01 |\n",
      "scGPT - INFO - | epoch  11 | 200/442 batches | lr 0.0000 | ms/batch 313.15 | loss 204.12 | mse 96.04 | mre 2696657.90 |gepc 98.45 |\n",
      "scGPT - INFO - | epoch  11 | 300/442 batches | lr 0.0000 | ms/batch 312.84 | loss 162.54 | mse 75.03 | mre 2144162.63 |gepc 76.05 |\n",
      "scGPT - INFO - | epoch  11 | 400/442 batches | lr 0.0000 | ms/batch 305.64 | loss 179.74 | mse 78.85 | mre 2214603.21 |gepc 88.85 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 142.18s | valid loss/mse 93.6909 | mre 2582235.6571\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.6909\n",
      "random masking at epoch  12, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  12 | 100/442 batches | lr 0.0000 | ms/batch 305.50 | loss 268.09 | mse 128.21 | mre 3316213.22 |gepc 130.50 |\n",
      "scGPT - INFO - | epoch  12 | 200/442 batches | lr 0.0000 | ms/batch 309.57 | loss 203.30 | mse 96.00 | mre 2704573.95 |gepc 97.66 |\n",
      "scGPT - INFO - | epoch  12 | 300/442 batches | lr 0.0000 | ms/batch 298.76 | loss 162.63 | mse 74.88 | mre 2134659.62 |gepc 76.37 |\n",
      "scGPT - INFO - | epoch  12 | 400/442 batches | lr 0.0000 | ms/batch 302.30 | loss 172.89 | mse 78.71 | mre 2217059.00 |gepc 82.18 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 139.62s | valid loss/mse 93.1371 | mre 2368445.1544\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.1371\n",
      "random masking at epoch  13, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  13 | 100/442 batches | lr 0.0000 | ms/batch 307.58 | loss 266.35 | mse 127.36 | mre 3323580.90 |gepc 129.81 |\n",
      "scGPT - INFO - | epoch  13 | 200/442 batches | lr 0.0000 | ms/batch 309.14 | loss 203.15 | mse 96.03 | mre 2659011.84 |gepc 97.66 |\n",
      "scGPT - INFO - | epoch  13 | 300/442 batches | lr 0.0000 | ms/batch 306.59 | loss 161.84 | mse 74.60 | mre 2144074.44 |gepc 75.79 |\n",
      "scGPT - INFO - | epoch  13 | 400/442 batches | lr 0.0000 | ms/batch 304.60 | loss 176.04 | mse 79.17 | mre 2231363.38 |gepc 85.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 140.48s | valid loss/mse 92.8299 | mre 2472118.7551\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 92.8299\n",
      "random masking at epoch  14, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  14 | 100/442 batches | lr 0.0000 | ms/batch 308.13 | loss 267.77 | mse 127.31 | mre 3302026.74 |gepc 131.54 |\n",
      "scGPT - INFO - | epoch  14 | 200/442 batches | lr 0.0000 | ms/batch 314.23 | loss 202.64 | mse 95.75 | mre 2671576.04 |gepc 97.51 |\n",
      "scGPT - INFO - | epoch  14 | 300/442 batches | lr 0.0000 | ms/batch 305.98 | loss 161.98 | mse 74.71 | mre 2132258.21 |gepc 76.11 |\n",
      "scGPT - INFO - | epoch  14 | 400/442 batches | lr 0.0000 | ms/batch 303.70 | loss 171.33 | mse 78.38 | mre 2245792.00 |gepc 82.13 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 141.02s | valid loss/mse 92.8840 | mre 2513042.1166\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  15, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  15 | 100/442 batches | lr 0.0000 | ms/batch 308.06 | loss 265.64 | mse 127.06 | mre 3307406.49 |gepc 129.77 |\n",
      "scGPT - INFO - | epoch  15 | 200/442 batches | lr 0.0000 | ms/batch 311.47 | loss 201.77 | mse 95.67 | mre 2693886.79 |gepc 97.25 |\n",
      "scGPT - INFO - | epoch  15 | 300/442 batches | lr 0.0000 | ms/batch 301.15 | loss 159.98 | mse 74.17 | mre 2098425.01 |gepc 76.11 |\n",
      "scGPT - INFO - | epoch  15 | 400/442 batches | lr 0.0000 | ms/batch 309.33 | loss 171.87 | mse 78.31 | mre 2234919.42 |gepc 83.48 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 140.84s | valid loss/mse 92.8037 | mre 2606905.1222\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 92.8037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482/482 [00:45<00:00, 10.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train/dab</td><td>▄▁▃▇▂▇▁▅▅▅▇▄▆▅▆█▄▄▅▂▄▄▄▅▅▄▄▂▄▄▄▄▅▃▄▂▄▃▅▅</td></tr><tr><td>train/ecs</td><td>███▆█▄▄█▄▇▆▃▅▃▆▆▃▆▃▃▄▃▄▃▂▆▃▁▄▂▃▃▁▄▅▁▄▁▄▃</td></tr><tr><td>train/mse</td><td>█▂▂▄▂▃▃▁▃▁▂▃▂▃▁▂▃▁▂▃▂▂▂▂▃▁▂▃▂▃▁▂▃▁▂▃▁▃▁▁</td></tr><tr><td>train/mvc</td><td>█▃▂▆▂▆▄▂▅▂▄▅▂▅▂▃▅▁▂▅▂▂▂▂▅▂▂▄▂▄▂▂▄▂▂▄▁▄▁▂</td></tr><tr><td>train/mvc_nzlp</td><td>▄▂▂▃▁▄▂▂▃▁▂▂▁█▃▂▄▁▁▂▁▁▁▁▃▁▁▂▁▂▁▁▃▁▁▂▁▂▁▁</td></tr><tr><td>train/nzlp</td><td>█▇▆▇▄▅▅▂▅▁▂▄▂▄▂▂▄▁▃▄▂▃▂▂▄▂▂▄▂▄▂▂▄▂▂▄▂▄▁▂</td></tr><tr><td>valid/dab</td><td>█▆▅▄▃▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid/mre</td><td>█▂▁▇▅▄▁▂▄▄▃▂▂▃▃</td></tr><tr><td>valid/mse</td><td>█▄▃▃▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid/sum_mse_dab</td><td>█▄▃▃▃▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train/dab</td><td>2.46666</td></tr><tr><td>train/ecs</td><td>6.56948</td></tr><tr><td>train/mse</td><td>76.07622</td></tr><tr><td>train/mvc</td><td>78.41503</td></tr><tr><td>train/mvc_nzlp</td><td>0.44025</td></tr><tr><td>train/nzlp</td><td>0.39032</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">gentle-microwave-50</strong>: <a href=\"https://wandb.ai/madlab_sdu/scGPT/runs/134qqtnn\" target=\"_blank\">https://wandb.ai/madlab_sdu/scGPT/runs/134qqtnn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230829_135103-134qqtnn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/NMF_project/reproducibility/wandb/run-20230829_142752-2ts65esa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/madlab_sdu/scGPT/runs/2ts65esa\" target=\"_blank\">stilted-rain-51</a></strong> to <a href=\"https://wandb.ai/madlab_sdu/scGPT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'Mixture', 'do_train': True, 'load_model': '/work/NMF_project/reproducibility/data/scGPT/Model/', 'GEPC': True, 'ecs_thres': 0.8, 'dab_weight': 1.0, 'mask_ratio': 0.4, 'epochs': 15, 'n_bins': 51, 'lr': 0.0001, 'batch_size': 64, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True}\n",
      "save to /work/NMF_project/reproducibility/data/scGPT/FT/dev_Mixture_4\n",
      "scGPT - INFO - match 13123/13760 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /work/NMF_project/reproducibility/data/scGPT/Model/best_model.pt, the model args will be overriden by the config /work/NMF_project/reproducibility/data/scGPT/Model/args.json.\n",
      "scGPT - INFO - train set number of samples: 27747, \n",
      "\t feature length: 1201\n",
      "scGPT - INFO - valid set number of samples: 3083, \n",
      "\t feature length: 1201\n",
      "Use domain specific batchnorm with affine=False\n",
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.bias with shape torch.Size([512])\n",
      "random masking at epoch   1, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   1 | 100/442 batches | lr 0.0001 | ms/batch 303.79 | loss 499.29 | mse 248.53 | mre 5207352.70 |gepc 236.31 |\n",
      "scGPT - INFO - | epoch   1 | 200/442 batches | lr 0.0001 | ms/batch 309.15 | loss 265.72 | mse 117.73 | mre 3315810.21 |gepc 134.03 |\n",
      "scGPT - INFO - | epoch   1 | 300/442 batches | lr 0.0001 | ms/batch 306.17 | loss 189.82 | mse 84.09 | mre 2465314.33 |gepc 91.48 |\n",
      "scGPT - INFO - | epoch   1 | 400/442 batches | lr 0.0001 | ms/batch 303.31 | loss 194.61 | mse 86.59 | mre 2448745.97 |gepc 93.23 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 140.05s | valid loss/mse 121.4653 | mre 3457703.0238\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 121.4653\n",
      "random masking at epoch   2, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   2 | 100/442 batches | lr 0.0001 | ms/batch 301.62 | loss 323.51 | mse 141.60 | mre 3713795.67 |gepc 167.08 |\n",
      "scGPT - INFO - | epoch   2 | 200/442 batches | lr 0.0001 | ms/batch 308.85 | loss 238.22 | mse 102.56 | mre 2906366.79 |gepc 122.39 |\n",
      "scGPT - INFO - | epoch   2 | 300/442 batches | lr 0.0001 | ms/batch 299.90 | loss 181.21 | mse 79.38 | mre 2336443.26 |gepc 88.06 |\n",
      "scGPT - INFO - | epoch   2 | 400/442 batches | lr 0.0001 | ms/batch 298.89 | loss 197.34 | mse 85.33 | mre 2464687.97 |gepc 97.55 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 138.99s | valid loss/mse 106.0593 | mre 2455042.5792\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 106.0593\n",
      "random masking at epoch   3, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   3 | 100/442 batches | lr 0.0001 | ms/batch 303.27 | loss 298.16 | mse 135.36 | mre 3466317.40 |gepc 150.03 |\n",
      "scGPT - INFO - | epoch   3 | 200/442 batches | lr 0.0001 | ms/batch 320.56 | loss 222.82 | mse 100.80 | mre 2759825.69 |gepc 109.61 |\n",
      "scGPT - INFO - | epoch   3 | 300/442 batches | lr 0.0001 | ms/batch 302.89 | loss 182.01 | mse 79.09 | mre 2261563.75 |gepc 89.21 |\n",
      "scGPT - INFO - | epoch   3 | 400/442 batches | lr 0.0001 | ms/batch 309.58 | loss 210.23 | mse 82.92 | mre 2364756.66 |gepc 113.41 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 141.63s | valid loss/mse 102.8710 | mre 2033414.1761\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 102.8710\n",
      "random masking at epoch   4, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   4 | 100/442 batches | lr 0.0001 | ms/batch 308.80 | loss 286.91 | mse 133.25 | mre 3379963.29 |gepc 141.89 |\n",
      "scGPT - INFO - | epoch   4 | 200/442 batches | lr 0.0001 | ms/batch 312.52 | loss 216.67 | mse 99.15 | mre 2807899.60 |gepc 105.54 |\n",
      "scGPT - INFO - | epoch   4 | 300/442 batches | lr 0.0001 | ms/batch 302.61 | loss 185.74 | mse 78.38 | mre 2362415.13 |gepc 94.57 |\n",
      "scGPT - INFO - | epoch   4 | 400/442 batches | lr 0.0001 | ms/batch 304.33 | loss 191.91 | mse 81.63 | mre 2281576.35 |gepc 97.61 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 140.88s | valid loss/mse 103.0793 | mre 3359271.0539\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   5, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   5 | 100/442 batches | lr 0.0001 | ms/batch 308.43 | loss 287.72 | mse 131.68 | mre 3459756.37 |gepc 144.65 |\n",
      "scGPT - INFO - | epoch   5 | 200/442 batches | lr 0.0001 | ms/batch 312.53 | loss 215.27 | mse 98.21 | mre 2751980.02 |gepc 105.89 |\n",
      "scGPT - INFO - | epoch   5 | 300/442 batches | lr 0.0001 | ms/batch 307.36 | loss 181.85 | mse 77.05 | mre 2231564.47 |gepc 92.78 |\n",
      "scGPT - INFO - | epoch   5 | 400/442 batches | lr 0.0001 | ms/batch 304.96 | loss 189.31 | mse 80.83 | mre 2274394.45 |gepc 96.07 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 141.52s | valid loss/mse 97.6181 | mre 2408574.3199\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 97.6181\n",
      "random masking at epoch   6, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   6 | 100/442 batches | lr 0.0001 | ms/batch 312.79 | loss 276.51 | mse 130.71 | mre 3354356.14 |gepc 135.13 |\n",
      "scGPT - INFO - | epoch   6 | 200/442 batches | lr 0.0001 | ms/batch 322.98 | loss 215.67 | mse 98.05 | mre 2739375.66 |gepc 106.67 |\n",
      "scGPT - INFO - | epoch   6 | 300/442 batches | lr 0.0001 | ms/batch 301.34 | loss 167.21 | mse 76.03 | mre 2195261.78 |gepc 79.30 |\n",
      "scGPT - INFO - | epoch   6 | 400/442 batches | lr 0.0001 | ms/batch 309.12 | loss 185.83 | mse 79.90 | mre 2276212.86 |gepc 93.53 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 142.39s | valid loss/mse 95.9238 | mre 2483094.0671\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 95.9238\n",
      "random masking at epoch   7, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   7 | 100/442 batches | lr 0.0001 | ms/batch 304.19 | loss 275.11 | mse 130.18 | mre 3344406.67 |gepc 134.93 |\n",
      "scGPT - INFO - | epoch   7 | 200/442 batches | lr 0.0001 | ms/batch 307.00 | loss 210.54 | mse 98.11 | mre 2781495.26 |gepc 101.96 |\n",
      "scGPT - INFO - | epoch   7 | 300/442 batches | lr 0.0001 | ms/batch 305.62 | loss 175.57 | mse 76.38 | mre 2156667.10 |gepc 87.44 |\n",
      "scGPT - INFO - | epoch   7 | 400/442 batches | lr 0.0001 | ms/batch 313.75 | loss 192.75 | mse 79.68 | mre 2272370.57 |gepc 101.04 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 140.80s | valid loss/mse 96.2450 | mre 2878280.2961\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   8, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   8 | 100/442 batches | lr 0.0000 | ms/batch 310.96 | loss 272.98 | mse 129.56 | mre 3378898.34 |gepc 133.47 |\n",
      "scGPT - INFO - | epoch   8 | 200/442 batches | lr 0.0000 | ms/batch 308.27 | loss 207.60 | mse 97.27 | mre 2713508.73 |gepc 100.23 |\n",
      "scGPT - INFO - | epoch   8 | 300/442 batches | lr 0.0000 | ms/batch 303.88 | loss 168.02 | mse 75.75 | mre 2148679.19 |gepc 80.35 |\n",
      "scGPT - INFO - | epoch   8 | 400/442 batches | lr 0.0000 | ms/batch 313.19 | loss 179.33 | mse 79.35 | mre 2268063.42 |gepc 87.80 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 141.72s | valid loss/mse 95.1760 | mre 2449811.8543\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 95.1760\n",
      "random masking at epoch   9, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch   9 | 100/442 batches | lr 0.0000 | ms/batch 304.65 | loss 272.50 | mse 128.99 | mre 3391680.39 |gepc 133.59 |\n",
      "scGPT - INFO - | epoch   9 | 200/442 batches | lr 0.0000 | ms/batch 316.06 | loss 207.41 | mse 96.86 | mre 2688081.67 |gepc 100.47 |\n",
      "scGPT - INFO - | epoch   9 | 300/442 batches | lr 0.0000 | ms/batch 310.85 | loss 164.80 | mse 75.61 | mre 2223638.99 |gepc 77.49 |\n",
      "scGPT - INFO - | epoch   9 | 400/442 batches | lr 0.0000 | ms/batch 302.56 | loss 184.53 | mse 79.37 | mre 2218442.20 |gepc 93.12 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 141.14s | valid loss/mse 94.9229 | mre 2693927.6401\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 94.9229\n",
      "random masking at epoch  10, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  10 | 100/442 batches | lr 0.0000 | ms/batch 306.38 | loss 270.54 | mse 128.21 | mre 3362007.81 |gepc 132.58 |\n",
      "scGPT - INFO - | epoch  10 | 200/442 batches | lr 0.0000 | ms/batch 313.74 | loss 206.22 | mse 96.44 | mre 2727363.04 |gepc 99.78 |\n",
      "scGPT - INFO - | epoch  10 | 300/442 batches | lr 0.0000 | ms/batch 299.84 | loss 162.83 | mse 74.87 | mre 2155711.65 |gepc 76.56 |\n",
      "scGPT - INFO - | epoch  10 | 400/442 batches | lr 0.0000 | ms/batch 304.26 | loss 175.76 | mse 79.10 | mre 2231432.65 |gepc 84.44 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 140.68s | valid loss/mse 95.0578 | mre 2827276.1308\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  11, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  11 | 100/442 batches | lr 0.0000 | ms/batch 308.13 | loss 268.22 | mse 127.93 | mre 3334676.15 |gepc 130.77 |\n",
      "scGPT - INFO - | epoch  11 | 200/442 batches | lr 0.0000 | ms/batch 314.21 | loss 205.62 | mse 96.31 | mre 2713122.13 |gepc 99.31 |\n",
      "scGPT - INFO - | epoch  11 | 300/442 batches | lr 0.0000 | ms/batch 305.15 | loss 162.59 | mse 75.06 | mre 2157590.46 |gepc 76.18 |\n",
      "scGPT - INFO - | epoch  11 | 400/442 batches | lr 0.0000 | ms/batch 306.21 | loss 174.19 | mse 78.58 | mre 2191743.10 |gepc 83.61 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 141.20s | valid loss/mse 93.5865 | mre 2559253.2133\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.5865\n",
      "random masking at epoch  12, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  12 | 100/442 batches | lr 0.0000 | ms/batch 308.30 | loss 270.73 | mse 128.27 | mre 3291924.90 |gepc 132.94 |\n",
      "scGPT - INFO - | epoch  12 | 200/442 batches | lr 0.0000 | ms/batch 314.99 | loss 204.77 | mse 96.12 | mre 2715866.34 |gepc 98.79 |\n",
      "scGPT - INFO - | epoch  12 | 300/442 batches | lr 0.0000 | ms/batch 302.57 | loss 162.43 | mse 75.06 | mre 2152669.27 |gepc 76.04 |\n",
      "scGPT - INFO - | epoch  12 | 400/442 batches | lr 0.0000 | ms/batch 309.65 | loss 173.64 | mse 78.45 | mre 2157458.28 |gepc 83.25 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 141.54s | valid loss/mse 93.9039 | mre 2723548.8568\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  13, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  13 | 100/442 batches | lr 0.0000 | ms/batch 305.35 | loss 268.82 | mse 128.15 | mre 3370174.13 |gepc 131.28 |\n",
      "scGPT - INFO - | epoch  13 | 200/442 batches | lr 0.0000 | ms/batch 311.72 | loss 203.68 | mse 96.06 | mre 2655506.95 |gepc 97.95 |\n",
      "scGPT - INFO - | epoch  13 | 300/442 batches | lr 0.0000 | ms/batch 306.88 | loss 162.79 | mse 74.40 | mre 2169084.05 |gepc 77.08 |\n",
      "scGPT - INFO - | epoch  13 | 400/442 batches | lr 0.0000 | ms/batch 308.97 | loss 174.67 | mse 78.29 | mre 2213730.52 |gepc 84.60 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 141.62s | valid loss/mse 93.4998 | mre 2636933.2600\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.4998\n",
      "random masking at epoch  14, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  14 | 100/442 batches | lr 0.0000 | ms/batch 308.36 | loss 267.93 | mse 127.52 | mre 3314569.69 |gepc 131.17 |\n",
      "scGPT - INFO - | epoch  14 | 200/442 batches | lr 0.0000 | ms/batch 313.39 | loss 202.73 | mse 95.71 | mre 2687498.06 |gepc 97.60 |\n",
      "scGPT - INFO - | epoch  14 | 300/442 batches | lr 0.0000 | ms/batch 309.21 | loss 160.26 | mse 74.16 | mre 2138426.59 |gepc 75.44 |\n",
      "scGPT - INFO - | epoch  14 | 400/442 batches | lr 0.0000 | ms/batch 309.26 | loss 170.24 | mse 78.33 | mre 2231661.43 |gepc 81.59 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 141.87s | valid loss/mse 93.1194 | mre 2572787.0043\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 93.1194\n",
      "random masking at epoch  15, ratio of masked values in train:  0.3997\n",
      "scGPT - INFO - | epoch  15 | 100/442 batches | lr 0.0000 | ms/batch 306.13 | loss 267.42 | mse 127.60 | mre 3305291.83 |gepc 130.72 |\n",
      "scGPT - INFO - | epoch  15 | 200/442 batches | lr 0.0000 | ms/batch 308.09 | loss 203.03 | mse 95.86 | mre 2709290.25 |gepc 98.27 |\n",
      "scGPT - INFO - | epoch  15 | 300/442 batches | lr 0.0000 | ms/batch 302.62 | loss 159.46 | mse 74.18 | mre 2149354.24 |gepc 75.79 |\n",
      "scGPT - INFO - | epoch  15 | 400/442 batches | lr 0.0000 | ms/batch 309.53 | loss 170.21 | mse 78.19 | mre 2212820.76 |gepc 82.08 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 140.70s | valid loss/mse 92.5918 | mre 2473618.2964\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 92.5918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 482/482 [00:45<00:00, 10.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train/dab</td><td>▄▁▂▇▃▆▃▅▄▅▄▄█▄▆▇▄▃▅▂▆▄▄▅▄▄▃▃▄▄▄▄▆▃▅▂▄▃▅▅</td></tr><tr><td>train/ecs</td><td>███▇█▄▄█▄▇▅▃▅▃▆▆▂▆▃▃▄▃▄▃▂▆▃▁▄▂▃▃▁▄▅▁▃▁▃▃</td></tr><tr><td>train/mse</td><td>█▂▂▄▂▃▃▁▃▁▂▃▂▃▂▂▃▁▂▃▂▂▂▂▃▁▂▃▂▃▁▂▃▁▂▃▁▃▁▁</td></tr><tr><td>train/mvc</td><td>█▃▂▆▂▅▄▂▅▂▂▅▅▅▂▃▅▁▂▄▂▂▂▂▅▂▂▄▂▅▂▂▅▂▂▄▁▄▁▂</td></tr><tr><td>train/mvc_nzlp</td><td>▃▂▂▂▁▂▂▁▂▁▁▂▂▄▁▁▂▁▁▂▁▁▁▁▂▁▁▂▁▂▁▁█▁▁▁▁▁▁▁</td></tr><tr><td>train/nzlp</td><td>█▇▆▇▄▅▅▂▄▁▂▄▃▄▂▂▄▁▃▄▂▃▂▂▄▂▂▄▂▄▂▂▄▂▂▄▂▄▁▂</td></tr><tr><td>valid/dab</td><td>█▆▄▆▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid/mre</td><td>█▃▁█▃▃▅▃▄▅▄▄▄▄▃</td></tr><tr><td>valid/mse</td><td>█▄▃▄▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>valid/sum_mse_dab</td><td>█▄▄▄▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train/dab</td><td>2.32589</td></tr><tr><td>train/ecs</td><td>6.58555</td></tr><tr><td>train/mse</td><td>76.10165</td></tr><tr><td>train/mvc</td><td>78.42239</td></tr><tr><td>train/mvc_nzlp</td><td>0.43788</td></tr><tr><td>train/nzlp</td><td>0.39053</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">stilted-rain-51</strong>: <a href=\"https://wandb.ai/madlab_sdu/scGPT/runs/2ts65esa\" target=\"_blank\">https://wandb.ai/madlab_sdu/scGPT/runs/2ts65esa</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230829_142752-2ts65esa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Define variables\n",
    "dataset_id = \"Mixture\"\n",
    "adata_path = \"/work/NMF_project/reproducibility/data/Mixture/mix.h5ad\"\n",
    "hvg_path = \"/work/NMF_project/reproducibility/data/Mixture/mix.features\"\n",
    "latent_path = \"/work/NMF_project/reproducibility/data/Mixture/mix\"\n",
    "\n",
    "for rep in range(5):\n",
    "    ## Set hyperparameters\n",
    "    hyperparameter_defaults = dict(\n",
    "        seed=42,\n",
    "        dataset_name=dataset_id, # Dataset name\n",
    "        do_train=True, # Flag to indicate whether to do update model parameters during training\n",
    "        load_model=\"/work/NMF_project/reproducibility/data/scGPT/Model/\", # Path to pre-trained model\n",
    "        GEPC=True,  # Gene expression modelling for cell objective\n",
    "        ecs_thres=0.8,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "        dab_weight=1.0, # DAR objective weight for batch correction\n",
    "        mask_ratio=0.4, # Default mask ratio\n",
    "        epochs=15, # Default number of epochs for fine-tuning\n",
    "        n_bins=51, # Default number of bins for value binning in data pre-processing\n",
    "        lr=1e-4, # Default learning rate for fine-tuning\n",
    "        batch_size=64, # Default batch size for fine-tuning\n",
    "        layer_size=128,\n",
    "        nlayers=4,\n",
    "        nhead=4, # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "        dropout=0.2, # Default dropout rate during model fine-tuning\n",
    "        schedule_ratio=0.9,  # Default rate for learning rate decay\n",
    "        save_eval_interval=5, # Default model evaluation interval\n",
    "        log_interval=100, # Default log interval\n",
    "        fast_transformer=True, # Default setting\n",
    "        pre_norm=False, # Default setting\n",
    "        amp=True,  # # Default setting: Automatic Mixed Precision\n",
    "    )\n",
    "\n",
    "    ## Initialize the run on wandb\n",
    "    run = wandb.init(\n",
    "        config=hyperparameter_defaults,\n",
    "        project=\"scGPT\",\n",
    "        reinit=True,\n",
    "        settings=wandb.Settings(start_method=\"fork\"),\n",
    "    )\n",
    "    config = wandb.config\n",
    "    print(config)\n",
    "    set_seed(config.seed)\n",
    "\n",
    "    # Settings for input and preprocessing\n",
    "    pad_token = \"<pad>\"\n",
    "    special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "    mask_ratio = config.mask_ratio\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = config.n_bins\n",
    "    n_hvg = 1200  # number of highly variable genes\n",
    "    max_seq_len = n_hvg + 1\n",
    "    per_seq_batch_sample = True\n",
    "    DSBN = True  # Domain-spec batchnorm\n",
    "    explicit_zero_prob = True  # whether explicit bernoulli for zeros\n",
    "\n",
    "    # Settings for saving the model\n",
    "    dataset_name = config.dataset_name\n",
    "    save_dir = Path(f\"/work/NMF_project/reproducibility/data/scGPT/FT/dev_{dataset_name}_{rep}/\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"save to {save_dir}\")\n",
    "    logger = scg.logger\n",
    "    scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "\n",
    "    # Load data\n",
    "    adata = sc.read(adata_path)\n",
    "    ori_batch_col = \"batch_label\"\n",
    "    adata.var = adata.var.set_index(\"features\")\n",
    "    data_is_raw = True\n",
    "\n",
    "    # make the batch category column\n",
    "    adata.obs[\"str_batch\"] = adata.obs[ori_batch_col].astype(str)\n",
    "    batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "    adata.obs[\"batch_id\"] = batch_id_labels\n",
    "    adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "\n",
    "    # Define HVGs\n",
    "    f = open(hvg_path, \"r\")\n",
    "    hvg = f.read().splitlines()\n",
    "    f.close()\n",
    "    adata.var['highly_variable'] = [True if g in hvg else False for g in adata.var_names]\n",
    "\n",
    "    # Load the pretrained model\n",
    "    if config.load_model is not None:\n",
    "        model_dir = Path(config.load_model)\n",
    "        model_config_file = model_dir / \"args.json\"\n",
    "        model_file = model_dir / \"best_model.pt\"\n",
    "        vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "        vocab = GeneVocab.from_file(vocab_file)\n",
    "        for s in special_tokens:\n",
    "            if s not in vocab:\n",
    "                vocab.append_token(s)\n",
    "\n",
    "        adata.var[\"id_in_vocab\"] = [\n",
    "            1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "        ]\n",
    "        gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "        logger.info(\n",
    "            f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "            f\"in vocabulary of size {len(vocab)}.\"\n",
    "        )\n",
    "        adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "        # model\n",
    "        with open(model_config_file, \"r\") as f:\n",
    "            model_configs = json.load(f)\n",
    "        logger.info(\n",
    "            f\"Resume model from {model_file}, the model args will be overriden by the \"\n",
    "            f\"config {model_config_file}.\"\n",
    "        )\n",
    "        embsize = model_configs[\"embsize\"]\n",
    "        nhead = model_configs[\"nheads\"]\n",
    "        d_hid = model_configs[\"d_hid\"]\n",
    "        nlayers = model_configs[\"nlayers\"]\n",
    "        n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "    else:\n",
    "        embsize = config.layer_size\n",
    "        nhead = config.nhead\n",
    "        nlayers = config.nlayers\n",
    "        d_hid = config.layer_size\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    sc.pp.filter_genes(adata, min_counts=3)\n",
    "    normed = sc.pp.normalize_total(adata, target_sum=1e4, layer=None, inplace=False)[\"X\"]\n",
    "    sc.get._set_obs_rep(adata, normed, layer=\"X_normed\")\n",
    "    sc.get._set_obs_rep(adata,sc.get._get_obs_rep(adata, layer=\"X_normed\"), layer=\"X_log1p\")\n",
    "    sc.pp.log1p(adata, layer=\"X_log1p\")\n",
    "    adata = adata[:, adata.var.highly_variable]\n",
    "    n_bins = config.n_bins  # NOTE: the first bin is always a spectial for zero\n",
    "    binned_rows = []\n",
    "    bin_edges = []\n",
    "    layer_data = sc.get._get_obs_rep(adata, layer=\"X_log1p\")\n",
    "    layer_data = layer_data.A if issparse(layer_data) else layer_data\n",
    "    for row in layer_data:\n",
    "        non_zero_ids = row.nonzero()\n",
    "        non_zero_row = row[non_zero_ids]\n",
    "        bins = np.quantile(non_zero_row, np.linspace(0, 1, n_bins - 1))\n",
    "        non_zero_digits = _digitize(x = non_zero_row, bins = bins)\n",
    "        assert non_zero_digits.min() >= 1\n",
    "        assert non_zero_digits.max() <= n_bins - 1\n",
    "        binned_row = np.zeros_like(row, dtype=np.int64)\n",
    "        binned_row[non_zero_ids] = non_zero_digits\n",
    "        binned_rows.append(binned_row)\n",
    "        bin_edges.append(np.concatenate([[0], bins]))\n",
    "    adata.layers[\"X_binned\"] = np.stack(binned_rows)\n",
    "    adata.obsm[\"bin_edges\"] = np.stack(bin_edges)\n",
    "\n",
    "    # Sort the adata by batch_id in advance\n",
    "    if per_seq_batch_sample:\n",
    "        adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()\n",
    "\n",
    "    # Define input layers and get counts\n",
    "    input_layer_key = \"X_binned\"\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "    genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "    # Get batch ids\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    num_batch_types = len(set(batch_ids))\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Create splits\n",
    "    (\n",
    "        train_data,\n",
    "        valid_data,\n",
    "        train_batch_labels,\n",
    "        valid_batch_labels,\n",
    "    ) = train_test_split(\n",
    "        all_counts, batch_ids, test_size=0.1, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Define vocabulary\n",
    "    if config.load_model is None:\n",
    "        vocab = Vocab(\n",
    "            VocabPybind(genes + special_tokens, None)\n",
    "        )  # bidirectional lookup [gene <-> int]\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    gene_ids = np.array(vocab(genes), dtype=int)\n",
    "\n",
    "    # Tokenize training and validation data\n",
    "    tokenized_train = tokenize_and_pad_batch(\n",
    "        train_data,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    tokenized_valid = tokenize_and_pad_batch(\n",
    "        valid_data,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "        f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "        f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    "    )\n",
    "\n",
    "    # Load the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ntokens = len(vocab)  # size of vocabulary\n",
    "    model = TransformerModel(\n",
    "        ntokens,\n",
    "        embsize,\n",
    "        nhead,\n",
    "        d_hid,\n",
    "        nlayers,\n",
    "        vocab=vocab,\n",
    "        dropout=config.dropout,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        do_mvc=config.GEPC,\n",
    "        do_dab=True,\n",
    "        use_batch_labels=True,\n",
    "        num_batch_labels=num_batch_types,\n",
    "        domain_spec_batchnorm=DSBN,\n",
    "        n_input_bins=n_input_bins,\n",
    "        ecs_threshold=config.ecs_thres,\n",
    "        explicit_zero_prob=explicit_zero_prob,\n",
    "        use_fast_transformer=config.fast_transformer,\n",
    "        pre_norm=config.pre_norm,\n",
    "    )\n",
    "    if config.load_model is not None:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_file))\n",
    "            logger.info(f\"Loading all model params from {model_file}\")\n",
    "        except:\n",
    "            # only load params that are in the model and match the size\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = torch.load(model_file)\n",
    "            pretrained_dict = {\n",
    "                k: v\n",
    "                for k, v in pretrained_dict.items()\n",
    "                if k in model_dict and v.shape == model_dict[k].shape\n",
    "            }\n",
    "            for k, v in pretrained_dict.items():\n",
    "                logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "            model_dict.update(pretrained_dict)\n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    wandb.watch(model)\n",
    "\n",
    "    # Set model criteria\n",
    "    criterion = masked_mse_loss\n",
    "    criterion_dab = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.amp)\n",
    "\n",
    "    # Train the model\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_avg_bio = 0.0\n",
    "    best_model = None\n",
    "    define_wandb_metrcis()\n",
    "\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "        train_loader = prepare_dataloader(\n",
    "            train_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        valid_loader = prepare_dataloader(\n",
    "            valid_data_pt,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            intra_domain_shuffle=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        if config.do_train:\n",
    "            train(\n",
    "                model,\n",
    "                loader=train_loader,\n",
    "            )\n",
    "        val_loss, val_mre = evaluate(\n",
    "            model,\n",
    "            loader=valid_loader,\n",
    "        )\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        logger.info(\"-\" * 89)\n",
    "        logger.info(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n",
    "        )\n",
    "        logger.info(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_model_epoch = epoch\n",
    "            logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    # Extract embeddings\n",
    "    best_model.eval()\n",
    "    adata_t = adata_sorted\n",
    "    adata_t = adata_t.copy()\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "    tokenized_all = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=True,\n",
    "    )\n",
    "    all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "    src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "        cell_embeddings = best_model.encode_batch(\n",
    "            all_gene_ids,\n",
    "            all_values.float(),\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            batch_size=config.batch_size,\n",
    "            batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "            time_step=0,\n",
    "            return_np=True,\n",
    "        )\n",
    "    cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "        cell_embeddings, axis=1, keepdims=True\n",
    "    )\n",
    "    \n",
    "    # Save the embeddings\n",
    "    df = pd.DataFrame(cell_embeddings, index=adata_sorted.obs.index)\n",
    "    file_out = latent_path + \"_scGPT_\" + str(rep) + \".txt\"\n",
    "    df.to_csv(file_out)\n",
    "    \n",
    "    # Clean up\n",
    "    del adata\n",
    "    del adata_sorted\n",
    "    del adata_t\n",
    "    del best_model\n",
    "    del tokenized_all\n",
    "    del all_counts\n",
    "    del model\n",
    "    del tokenized_train\n",
    "    del tokenized_valid\n",
    "    \n",
    "    # End the logger and the run\n",
    "    run.finish()\n",
    "    wandb.finish()\n",
    "    gc.collect()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba9ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT",
   "language": "python",
   "name": "scgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
